\section{Snapshots of AI in science}\label{snapshots-of-ai-in-science}

Across domains, AI is being deployed to advance the frontiers of
science. The snapshots below introduce some current areas of research in
AI for science, and explore the issues raised by these research
projects. Across these snapshots, some common themes emerge:

\begin{itemize}
\item
  How can researchers most effectively combine observations, data-driven
  models, and physical models to enhance understanding of complex
  systems? To answer this question, methods are needed to integrate
  different types of model, operating across different levels of
  granularity, while managing the impact of the uncertainties that
  emerge when a machine learning model is integrated in a wider system.
  New approaches to simulation and emulation can support progress in
  tackling these challenges, alongside new strategies for examining the
  robustness or performance of machine learning models.
\item
  How do the outputs from an AI system align with what researchers
  already know about the world, and how can such systems help uncover
  causal relationships in data? Advances in causal machine learning are
  needed to connect the laws and principles already established in many
  areas of research with data-driven methods.
\item
  How can AI be integrated into the scientific process safely and
  robustly? Effective integration will rely on the ability to encode
  domain knowledge in AI systems, the design of interfaces that
  facilitate interaction between humans and AI, and the development of
  mechanisms for sharing knowledge and know-how about how to use AI in
  practice.
\end{itemize}


\subsection{In Earth Sciences}\label{in-earth-sciences}

\textbf{The Earth is a complex system},\footnote{This example is
  inspired by Markus Reichstein's talk, the abstract for which is
  provided later in this document.} comprised of terrestrial, marine,
and atmospheric biospheres that interact with each other and are shaped
by biological, chemical, and physical processes that exchange energy
across scales from the molecular to the planetary. It is also a unique
system: researchers have yet to discover other planets that replicate
its dynamics. Studies of the Earth system therefore rely on observations
and physical models, which describe the dynamics of energy exchange from
first principles and use those principles to build models of the Earth's
sub-systems. As climate change perturbs this complex system, it is
increasingly important to have accurate models that can be used to
analyse how the Earth will respond to increasing carbon dioxide levels.
The challenge for Earth system science is to build more complex models
that represent the web of relationships between biospheres under
changing conditions, without generating overwhelming uncertainties and
while generating actionable insights that can be used by individuals,
organisations, and policymakers to understand the localised impact of
changing environmental conditions.\footnote{Summers, T.,~Mackie,
  E.,~Ueno, R.,~Simpson, C.,~Hosking, J. S.,~Suciu, T.,~Coburn, A., \&
  et al.~(2022) Localised impacts and economic implications from high
  temperature disruption days under climate change.~\emph{Climate
  Resilience and Sustainability~}\url{https://doi.org/10.1002/cli2.35}}

For example, how much carbon dioxide is absorbed by different biospheres
can be affected by diverse factors including volume and type of
vegetation cover, water and drought stress in different areas, and local
temperature, which have implications for how carbon dioxide contributes
to climate change. Researchers have access to data that describes local
uptake of carbon dioxide by some ecosystems, such as tropical
rainforest, European beech forest, or Mediterranean savanna, for
example, but lack sufficient observational coverage to scale from these
local observations to accurate global representations of carbon
exchange. One response to this challenge is to leverage data-driven
models to knit together the different mechanistic models that describe
(for example) carbon, water, and energy cycles in different biospheres.

By starting with observational data and combining this with
physics-informed modelling, researchers can leverage machine learning to
create simulations that can generate new understandings of how complex
systems function. Taking this approach, the FLUXNET project combines
observed data on carbon emissions from different sources to generate a
data-driven picture of global carbon dynamics. By combining data across
scales to establish a statistical model of global carbon dynamics, this
project can generate simulations of how the Earth breathes.\footnote{Beer,
  C., Reichstein, M., Tomelleri, E., Ciais, P., Jung, M., Carvalhais,
  N., Rodenbeck, C., Arain, M.A., Baldocchi, D., Bonan, G.B., Bondeau,
  A., Cescatti, A., Lasslop, G., Lindroth, A., Lomas, M., Luyssaert, S.,
  Margolis, H., Oleson, K.W., Roupsard, O., Veenendaal, E., Viovy, N.,
  Williams, C., Woodward, F.I., Papale, D. (2010) Terrestrial gross
  carbon dioxide uptake: global distribution and covariation with
  climate, Science, 329 (5993) (2010), pp. 834-838, DOI:
  10.1126/science.1184984} The ability to integrate across scales and
combine models of different Earth sub-systems can also contribute to
wider efforts to build a `digital twin' of the Earth, with the aim of
better understanding the implications of climate change across
biospheres and communities.

\textbf{As the Earth's climate changes},\footnote{This example is
  inspired by Markus Reichstein's talk, the abstract for which is
  provided later in this document.} researchers anticipate that local
environmental conditions will change and extreme weather events will
increase. Understanding the impact of these changes is important for
those seeking to develop appropriate responses, for example developing
environmental management plans or planning human activities.

How a landscape responds to changing environmental conditions will vary
depending on the local climate, characteristics of the terrain
(vegetation type, for example), and human activities in the area. Under
changing climate conditions, as extrapolation beyond known limits
becomes necessary, the assumptions or abstractions that form the basis
of a model can be rendered invalid. Relying solely on either mechanistic
descriptions of the system -- the impact of temperature on plant growth,
for example\footnote{Under conditions of extreme temperature, patterns
  of stomatal opening and closing in plants changes. See, for example:
  Marchin, R. M., Backes, D., Ossola, A., Leishman, M. R., Tjoelker, M.
  G., \& Ellsworth, D. S. (2022). Extreme heat increases stomatal
  conductance and drought-induced mortality risk in vulnerable plant
  species. \emph{Global Change Biology}, 28, 1133-- 1146.
  https://doi.org/10.1111/gcb.15976} -- or statistical models could
result in inaccuracies. Machine learning can help respond to this
challenge, through the creation of hybrid models that combine an
understanding of the physical laws with model parameters learned from
data. Researchers often already have access to known physical parameters
for a system (for example, the equations that govern how water
evaporates to air). These parameters can be fed into a machine learning
model that will learn other patterns. Known equations specify the
chemical and physical processes; machine learning can then help
elucidate the other biological forces at play. Integrating this physical
structure in the model helps make it both more interpretable to the
domain scientists and more reliable in its predictions. The resulting
model can accurately forecast the impact of climate change on the
features of local landscapes, operating within the bounds set by the
laws of physics.\footnote{Requena-Mesa, C., Reichstein, M., Mahecha, M.,
  Kraft, B., Denzler, J. (2019). Predicting Landscapes from
  Environmental Conditions Using Generative Networks. In: Fink, G.,
  Frintrop, S., Jiang, X. (eds) Pattern Recognition. DAGM GCPR 2019.
  Lecture Notes in Computer Science(), vol 11824. Springer, Cham.
  https://doi.org/10.1007/978-3-030-33676-9\_14}

\textbf{Ice loss}\footnote{This example is inspired by Ieva
  Kazlauskaitė's talk, the abstract for which is provided later in this
  document.} has been the greatest contributor to sea-level rise in
recent decades.\footnote{IPCC (2019) IPCC Special Report on the Ocean
  and Cryosphere in a Changing Climate {[}H.-O. Pörtner, D.C. Roberts,
  V. Masson-Delmotte, P. Zhai, M. Tignor, E. Poloczanska, K. Mintenbeck,
  A. Alegría, M. Nicolai, A. Okem, J. Petzold, B. Rama, N.M. Weyer
  (eds.){]}. Cambridge University Press, Cambridge, UK and New York, NY,
  USA, pp. 3--35.
  https://doi.org/10.1017/9781009157964.001.\href{https://www.ipcc.ch/srocc/}{\uline{https://www.ipcc.ch/srocc/}}}
Large volumes of fresh water are stored as ice: NASA estimates that if
all the world's glaciers and ice sheets melted, sea levels globally
would rise by over 60 metres, flooding all coastal cities.\footnote{NASA,
  Understanding sea level, available from:
  \href{https://sealevel.nasa.gov/understanding-sea-level/global-sea-level/ice-melt}{\uline{https://sealevel.nasa.gov/understanding-sea-level/global-sea-level/ice-melt}}}
Researchers can estimate the contribution that melting ice makes to sea
level rise through mechanistic models that describe the underlying
physical processes (that turn ice to water) and through observational
data about the velocity of ice sheet movement. Machine learning could
offer a toolkit to make these models more accurate, connecting ice sheet
models to ocean and atmospheric models, and integrating different data
types in hybrid mechanistic-data models.

Efforts to build such models, however, illustrate the complexity of
designing tools to meet domain needs. Projects in this space have
considered emulating the ice sheet system -- or its individual
components -- to see if models could be run faster; though successful
methodologically, it has not been clear that such efforts address a
clear research need. Another approach is to use machine learning to
streamline simulations, for instance by identifying the most effective
level of granularity for different models (is a spatial breakdown of 5km
or 10km more interesting?). An important lesson from such collaborations
is the specificity of domain needs: machine learning is a tool for
research, but just because researchers have a hammer, does not mean
every research problem is a nail. Effectively deploying machine learning
for research requires both suitable AI toolkits and an understanding of
which toolkits are best deployed for which challenges.

\subsection{In environmental and agricultural
sciences}\label{in-environmental-and-agricultural-sciences}

\textbf{Poultry farming}\footnote{This example is inspired by Dina
  Machuve's talk, the abstract for which is provided later in this
  document.} is a vital source of income and food for many communities
in Tanzania. 4.6 million households in the country raise approximately
36 million chickens, but despite the importance of this activity,
poultry farming suffers from relatively low productivity due to the
prevalence of disease. Efforts to tackle poultry diseases such as
Salmonella, Newcastle disease, and coccidiosis are held back by the
accessibility of diagnostic processes and lack of data. Diagnosis
currently requires lab analysis of droppings, which can take 3-4 days.
Once disease is confirmed, farmers often lose their entire farm's flock.

Farm-level tests and diagnostics could increase the effectiveness of
disease surveillance and treatment, giving farmers rapid access to
information about the diseases affecting their flock and action plans
about how to manage outbreaks. With mobile phones ubiquitous across the
country -- there are almost 49 million mobile phone subscriptions in
Tanzania -- there are opportunities for new uses of local data to detect
disease outbreaks.

By collecting images of droppings from farms, researchers have been
creating a dataset to train a machine learning system that can identify
the symptoms of these diseases. Fecal images are taken on farms,
annotated with diagnostic information from agricultural disease experts
and the results of lab tests, then used to train an image recognition
system to automate the diagnosis process.\footnote{Machuve D, Nwankwo E,
  Mduma N, Mbelwa J. (2022) Poultry diseases diagnostics models using
  deep learning. Front Artif Intell. 5, 733345. doi:
  10.3389/frai.2022.733345. Erratum in: Front Artif Intell. 2022 Sep
  01;5:1016695. PMID: 35978651; PMCID: PMC9376463.} System robustness
and accuracy is vital, given the significant implications of a positive
diagnosis, and careful design is necessary to incentivise farmers to
make use of the app.

Collaboration with experts from different domains is central to
developing this system. Input from farmers is needed to collect data and
test the system in practice; from veterinary pathologists to help
annotate the data and ensure the system's accuracy; and from
technologists to develop an AI system that is effective in deployment as
an app on mobile phones. These collaborations also open opportunities
for new forms of citizen science, as farmers and local communities are
engaged in efforts to develop and maintain an open toolkit for disease
diagnosis, providing a gateway for communities to take ownership of
machine learning as a tool to serve their needs.

\textbf{Trees and forests}\footnote{This example is inspired by
  Christian Igel's talk, the abstract for which is provided later in
  this document.} play a crucial role in maintaining healthy ecosystems.
Despite this, an estimated ten million hectares of forest are lost
globally each year due to reforestation, with only around half of this
balanced by tree-planting efforts.\footnote{Ritchie, H. and Roser, M.
  (2021) - "Forests and Deforestation". Published online at
  OurWorldInData.org,
  https://ourworldindata.org/forests-and-deforestation} Africa
experienced an annual rate of forest loss of approximately 3.9 million
hectares per year from 2010-2020. This loss has implications for
biodiversity and people, with trees a vital contributor to ecosystem
services such as carbon storage, food provision, and shelter. In this
shifting landscape, understanding the number and distribution of trees
is important for the development of forestry management plans and for
understanding the carbon storage implications of changes to land use.

To estimate the number and biomass of trees in the West African Sahara
and Sahel, researchers have used satellite imagery of 90,000 trees from
400 sampling sites to create a labelled dataset for use in machine
learning. Using an image segmentation tool to identify the location of
trees, an automated system was able to count the number of trees, with
domain experts guiding the system to distinguish trees from surrounding
vegetation. This tree count can then be used to estimate the biomass of
trees in the area, and predict the amount of carbon they store; the
prediction is generated using allometric calculations, which translate
the properties of the tree to its carbon storage potential. In this
approach, machine learning measures the properties of the ecosystem from
satellite images, then these properties are used to feed mechanistic
models that describe the ecosystem's physical functions.\footnote{Brandt,
  M., Tucker, C.J., Kariryaa, A. et al. (2020) An unexpectedly large
  count of trees in the West African Sahara and Sahel. Nature 587,
  78--82. https://doi.org/10.1038/s41586-020-2824-5} This opens the
possibility of new tools to estimate tree cover, leveraging these
insights for more effective environmental management. However, in the
process, care is needed to manage the type and nature of the
uncertainties created by different modelling approaches. Different
allometric models, for example, can be more or less suited to different
types of tree cover,\footnote{Hiernaux, P. and Issoufou, B.H., Igel, C.,
  Kariryaa, A., Kourouma, M., Chave, J., Mougin, E. and Savadogo, P.
  (2023) Allometric Equations to Estimate the Dry Mass of Sahel Woody
  Plants from Very-High Resolution Satellite Imagery. Forest Ecology and
  Management, 529,
  \href{https://doi.org/10.1016/j.foreco.2022.120653}{\uline{https://doi.org/10.1016/j.foreco.2022.120653}}}
meaning that the method for estimating biomass from satellite imagery
can be subject to biases when applied across a large area. A small error
in the calculation of the biomass from one tree can have a cumulatively
large effect when that method is scaled to country-level. The type and
nature of such uncertainties need to be considered when a machine
learning model is used within a wider system.

\textbf{Vector borne diseases}\footnote{This example is inspired by
  Christian Igel's talk, the abstract for which is provided later in
  this document.} account for more than 17\% of diseases in people and
over 700,000 deaths annually.\footnote{Hernandez-Triana, L. and Bell, S.
  (2022) Taking the sting out of vector borne diseases, APHA Science
  Blog, available at:
  \href{https://aphascience.blog.gov.uk/2022/07/06/vector-borne-diseases/}{\uline{https://aphascience.blog.gov.uk/2022/07/06/vector-borne-diseases/}}}
Changes to the climate and patterns of land use, amongst other factors,
are bringing human populations into contact with new vectors of disease.
In Africa, for example, populations of mosquitoes carrying malaria that
might previously have been found mainly in rural areas are spreading
into cities.

Tools to characterise building features from satellite imagery have
already been developed and made available for use.\footnote{For example:
  Quinn, J. (2021) Mapping Africa's Buildings with Satellite Imagery,
  Google Research Blog, available at:
  \href{https://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html}{\uline{https://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html}}}
Leveraging these to analyse multi-scale data -- from household to
city-level -- researchers are investigating how the built environment
influences people's risk of contracting mosquito-borne disease. For
example, it has been found that the prevalence of mosquitos in an area
is related to the type of roofing used in construction; metal roofing
tends to be associated with lower mosquito prevalence, potentially due
to the high temperatures they attract during the day.\footnote{Lindsay,
  S.W., Jawara, M., Mwesigwa, J., Achan, J., Bayoh, N., Bradley, J.,
  Kandeh, B., Kirby, M.J., Knudsen, J., Macdonald, M., Pinder, M.,
  Tusting, L.S., Weiss, D.J., Wilson, A.L. and D'Alessandro, U. (2019)
  Reduced mosquito survival in metal-roof houses may contribute to a
  decline in malaria transmission in sub-Saharan Africa. Scientific
  Reports 9, 7770,
  \href{https://doi.org/10.1038/s41598-019-43816-0}{\uline{https://doi.org/10.1038/s41598-019-43816-0}}}
These insights can be deployed by policymakers in the development of
appropriate policy responses.\footnote{Royal Danish Academy (2022) New
  research to combat malaria mosquitoes in African metropolises,
  available at:
  \href{https://royaldanishacademy.com/news/ny-forskning-skal-bekaempe-malariamyg-i-afrikanske-storbyer}{\uline{https://royaldanishacademy.com/news/ny-forskning-skal-bekaempe-malariamyg-i-afrikanske-storbyer}}}

Decisions made on the basis of insights generated by machine learning
models will be influenced by the assumptions made in those models. In
the context of housing, for example, the decision about which type of
housing to identify as `at risk' or which building materials to flag as
`problematic' may have significant consequences for individuals or
communities. When those decisions are assimilated within a model or
analysis before a downstream `policy decision', the implications for
those communities of different courses of action may be obscured,
creating a risk of marginalising or disadvantaging individuals or
groups. The assumptions are built into the model, and how visible those
assumptions are made to different user groups, can have significant
social and scientific consequences.

\subsection{In physical sciences}\label{in-physical-sciences}

\textbf{Understanding the nature of dark matter}\footnote{This example
  is inspired by Siddharth Mishra Sharma's talk, as well as insights
  from Gilles Louppe's talk, the abstracts for which are provided later
  in this document.} is one of the biggest unsolved challenges of
particle physics today. The matter that researchers can measure using
cosmological observations makes up about 5\% of the Universe.\footnote{NASA,
  Dark Energy, Dark Matter, available at:
  \href{https://science.nasa.gov/astrophysics/focus-areas/what-is-dark-energy}{\uline{https://science.nasa.gov/astrophysics/focus-areas/what-is-dark-energy}}}
While not directly observable, evidence for the existence of dark matter
can be found in a variety of phenomena not otherwise accounted for by
currently known laws of physics: stars rotate around galaxies faster
than might be expected; the pattern of fluctuations in primordial
microwave observations indicate that there were sources of gravitation
in the early Universe beyond ordinary matter; light bends around galaxy
clusters due to gravitational effects from dark matter.~

Despite knowing that dark matter exists and that it plays an important
role in how the Universe formed, its particle composition or properties
remains unclear. Investigating these properties is the focus of
large-scale experimental studies, for example in particle
colliders.\footnote{For example: Aad, G., Abat, E., Abdallah, J.,
  Abdelalim, A. A., Abdesselam, A., Abdinov, O., Abi, B. A., Abolins,
  M., Abramowicz, H., Acerbi, E., Acharya, B. S., Achenbach, R., Ackers,
  M., Adams, D. L., Adamyan, F., Addy, T. N., Aderholz, M., Adorisio,
  C., Adragna, P., ... Zychacek, V. (2008). The ATLAS Experiment at the
  CERN Large Hadron Collider. Journal of Instrumentation, 3(S08003).
  https://doi.org/10.1088/1748-0221/3/08/S08003} A variety of data could
contain information about the properties of dark matter, from studies of
cosmic rays, cosmic microwave radiation, properties of stars,
gravitational lensing studies, and more. These datasets are complex:
they are typically high-dimensional, represent complex relationships
between the micro-physics and macro-phenomenon in a system, and may
contain artefacts or noise from the instruments used to collect them. To
make use of this data, researchers need to account for this complexity
and tether their models to assumptions about physical processes.

The challenge for machine learning in astro-particle physics research is
to extract insights about the particle composition of dark matter from
the macroscopic patterns that can be observed in the Universe. For
example, gravitational lensing is a phenomenon in which the pathway of
light traveling through the Universe is deflected due to the influence
of gravity from an intervening mass, distorting how this background
light is observed.\footnote{Mishra-Sharma, S. and Yang, G. (2022) Strong
  Lensing Source Reconstruction Using Continuous Neural Fields,
  \href{https://arxiv.org/abs/2206.14820}{\uline{arXiv:2206.14820}}\textbf{~{[}}astro-ph.CO{]},
  \url{https://doi.org/10.48550/arXiv.2206.14820}} Gravitational lensing
effects arising from dark matter clumps (``substructure'') could hold
information about the structure of dark matter at a microscopic level.
To infer the presence of substructure of these lensing systems,
researchers need models that describe the effect of dark matter,
ordinary matter, and the wider environment while simultaneously
modelling the form of the background light, which can be a
morphologically-complex galaxy. By letting a machine learning model,
like a neural network, describe the complex background light source, it
is possible to make predictions about how the light might appear after
being lensed with and also without the impact of dark matter clumps. By
performing many simulations considering various possibilities,
researchers can compare these with observations from telescopes and
understand which dark matter theories are compatible with the data.

Rapid progress in this field is generating a variety of models and
approaches. In its next wave of development, further research is needed
to test how trustworthy these methods are, by assessing their
performance in generating physically plausible results and robust
constraints on the properties of dark matter and other forms of new
physics.\footnote{Dvorkin, C., Mishra-Sharma, S., Nord, B., Villar, V.
  A., Avestruz, C., Bechtol, K., Ćiprijanović, A., Connolly, A. J.,
  Garrison, L. H., Narayan, G. and Villaescusa-Navarro, F. (2022)
  Machine learning and cosmology, arXiv:2203.08056 {[}hep-ph{]},
  \url{https://doi.org/10.48550/arXiv.2203.08056}}

\textbf{How particles move}\footnote{This example is inspired by
  Francisco Vargas's talk, the abstract for which is provided later in
  this document.} across their environment is a shared area of interest
for many domains. In chemistry, for example, researchers are often
interested in how molecules diffuse, and where they end up distributed,
based on the physical forces that shape their movement over time. The
analogy of particle movement can also be applied as an abstraction of
larger scale physical processes, such as in agent-based models for crowd
simulation.\footnote{Examples of agent-based models for crowd simulation
  include: Makinoshima, F., Oishi, Y. (2022) Crowd flow forecasting via
  agent-based simulations with sequential latent parameter estimation
  from aggregate observation. Sci Rep 12, 11168.
  \href{https://doi.org/10.1038/s41598-022-14646-4}{\uline{https://doi.org/10.1038/s41598-022-14646-4}}
  and Malleson, N., Minors, K., Kieu, L., Ward, J. A., West, A. and
  Heppenstall, A (2020) Simulating Crowds in Real Time with Agent-Based
  Modelling and a Particle Filter, Journal of Artificial Societies and
  Social Simulation 23 (3) 3
  \textless http://jasss.soc.surrey.ac.uk/23/3/3.html\textgreater. doi:
  10.18564/jasss.4266} In these systems the initial system state is
represented in an initial probability distribution, the scientific
objective can then also be represented as a target distribution. The
dynamics underpinning this diffusion are formalised mathematically in
the Schrödinger bridge problem. This long-standing problem is concerned
with finding the most likely paths along which particles move from their
starting distribution to their distribution at a defined point in time,
based on experimentally-observed start and end positions. In general,
finding analytic solutions to the Schrödinger bridge problem is
intractable, but machine learning tools are providing new approaches for
finding approximate numerical solutions that can be deployed across
domains.\footnote{Vargas, F., Thodoroff, P., Lawrence, N.D. and
  Lamacraft, A. (2021) Solving Schrödinger Bridges via Maximum
  Likelihood, arXiv:2106.02081 {[}stat.ML{]},
  https\uline{://doi.org/10.48550/arXiv.2106.02081}}

\subsection{\texorpdfstring{In biological sciences
}{In biological sciences }}\label{in-biological-sciences}

\textbf{The development and differentiation of cells into tissues and
organs}\footnote{This example is inspired by Maren Büttner's talk, the
  abstract for which is provided later in this document.} is a
complicated process, shaped by hormonal and genetic influences on cell
growth.\footnote{Krishnamurthy, K.V., Bahadur, B., John Adams, S.,
  Venkatasubramanian, P. (2015). Development and Organization of Cell
  Types and Tissues. In: Bahadur, B., Venkat Rajam, M., Sahijram, L.,
  Krishnamurthy, K. (eds) Plant Biology and Biotechnology. Springer, New
  Delhi.
  \href{https://doi.org/10.1007/978-81-322-2286-6_3}{\uline{https://doi.org/10.1007/978-81-322-2286-6\_3}}}
Advances in genomics have allowed researchers to characterise the
genetic material of different organisms; more recent progress in
single-cell genomics extends this ability to the single-cell level,
unlocking detailed analysis of how genetic activity determines cellular
function.

Single-cell RNA studies examine how ribonucleic acids (RNA) shape
cellular properties and development pathways. The RNA profiles created
by genetic sequencing techniques allow researchers to identify which
genes are active in a cell. The question for the field today is how to
move from these single-cell analyses to an atlas of cell development
that shows how cells specialise and form tissues or organs.

By combining statistical and machine learning techniques, researchers
can reconstruct the gene dynamics -- which genes are activated at which
time -- that influence cell development.\footnote{Haghverdi, L.,
  Büttner, M., Wolf, F. et al. (2016) Diffusion pseudotime robustly
  reconstructs lineage branching. Nat Methods 13, 845--848.
  \href{https://doi.org/10.1038/nmeth.3971}{\uline{https://doi.org/10.1038/nmeth.3971}}}
Cells in the small intestine, for example, undergo a pattern of
differentiation that takes them from their base state to highly
specialised units, able to variously secrete mucous, absorb nutrients,
or respond to hormones. By studying what genes are expressed in a cell
at an early stage, researchers can predict how the cell will specialise
and identify which genetic changes are associated with that
specialisation, opening opportunities to treat intestinal
diseases.\footnote{Böttcher A, Büttner M, Tritschler S, Sterr M, Aliluev
  A, Oppenländer L, Burtscher I, Sass S, Irmler M, Beckers J, Ziegenhain
  C, Enard W, Schamberger AC, Verhamme FM, Eickelberg O, Theis FJ,
  Lickert H. Non-canonical Wnt/PCP signalling regulates intestinal stem
  cell lineage priming towards enteroendocrine and Paneth cell fates.
  Nat Cell Biol. 2021 Jan;23(1):23-31. doi: 10.1038/s41556-020-00617-2.
  Epub 2021 Jan 4. Erratum in: Nat Cell Biol. 2021 May;23(5):566-576.
  PMID: 33398177. \url{https://doi.org/10.1038/s41556-020-00617-2}}

Building these models relies on effective data management. Lab processes
can inject artefacts into datasets, for example batch effects arising
from how cells were grown or harvested for study, which need to be
removed from data before analysis. Effective data correction maintains
biologically-relevant information, while removing noise from the data. A
variety of tools exist for this correction, including regression models,
dimensionality reduction, graph methods, and deep learning. For domain
researchers to be able to identify the tools that are useful for them,
benchmarking studies are vital in identifying the most effective data
integration method for their purpose.\footnote{Luecken, M.D., Büttner,
  M., Chaichoompu, K. et al. Benchmarking atlas-level data integration
  in single-cell genomics. Nat Methods 19, 41--50 (2022).
  \href{https://doi.org/10.1038/s41592-021-01336-8}{\uline{https://doi.org/10.1038/s41592-021-01336-8}}}
However, there remain open questions about how best to benchmark the
performance of a system when there are complex pipelines of analysis
involved. Understanding the end-to-end nature of an analytical pipeline
can be difficult, and new approaches to assessing performance may be
needed.

\textbf{To understand how the brain works},\footnote{This example is
  inspired by Jakob Macke's talk, the abstract for which is provided
  later in this document.} neuroscientists develop mathematical models
that describe the activity of individual neurons, and how these connect
across brain networks. Models on the mechanistic level take the form of
differential equations. These models are based on experimental data,
from experiments that examine how neurons respond to different signals
or perturbations. To build a computational model from this data, it is
first necessary to find which factors influence how a neuron acts,
creating a set of parameters that determine how the model works. This
process of finding parameters is often labour-intensive, relying on
trial-and-error, which limits researchers' ability to scale models
across complex neural networks. Machine learning can help streamline
that model definition process, by predicting which models are more
likely to be compatible with data. By automatically identifying model
parameters, researchers can rapidly develop simulations of complex
structures, such as brains or nervous systems in different
animals.\footnote{Gonçalves, P., Lueckmann, J.M., Deistler, M.,
  Nonnenmacher, M., Öcal, K., Bassetto, G., Chintaluri, C.,
  Podlaski,W.F., Haddad, S.A., Vogels, T.P., Greenberg, D., and Macke,
  J.H. (2020) Training deep neural density estimators to identify
  mechanistic models of neural dynamics eLife 9:e56261
  \href{https://doi.org/10.7554/eLife.56261}{\uline{https://doi.org/10.7554/eLife.56261}}}

\textbf{Box 1: Talks given during this workshop session}

\emph{\textbf{Markus Reichstein: Machine-learning-model-data-integration
for a better understanding of the Earth System}}

The Earth is a complex dynamic networked system. Machine learning, i.e.
derivation of computational models from data, has already made important
contributions to predict and understand components of the Earth system,
specifically in climate, remote sensing and environmental sciences. For
instance, classifications of land cover types, prediction of
land-atmosphere and ocean-atmosphere exchange, or detection of extreme
events have greatly benefited from these approaches. Such data-driven
information has already changed how Earth system models are evaluated
and further developed. However, many studies have not yet sufficiently
addressed and exploited dynamic aspects of systems, such as memory
effects for prediction and effects of spatial context, e.g. for
classification and change detection. In particular new developments in
deep learning offer great potential to overcome these limitations. Yet,
a key challenge and opportunity is to integrate (physical-biological)
system modelling approaches with machine learning into hybrid modelling
approaches, which combines physical consistency and machine learning
versatility. A couple of examples are given with focus on the
terrestrial biosphere, where the combination of system-based and
machine-learning-based modelling helps our understanding of aspects of
the Earth system.

\emph{\textbf{Dina Machuve: Poultry Diseases Diagnostics Models using
Deep Learning}}

Coccidiosis, Salmonella, and Newcastle are the common poultry diseases
that curtail poultry production if they are not detected early. In
Tanzania, these diseases are not detected early due to limited access to
agricultural support services by poultry farmers. Deep learning
techniques have the potential for early diagnosis of these poultry
diseases. In this study, a deep Convolutional Neural Network (CNN) model
was developed to diagnose poultry diseases by classifying healthy and
unhealthy fecal images. Unhealthy fecal images may be symptomatic of
Coccidiosis, Salmonella, and Newcastle diseases. We collected 1,255
laboratory-labeled fecal images and fecal samples used in Polymerase
Chain Reaction diagnostics to annotate the laboratory-labeled fecal
images. We took 6,812 poultry fecal photos using an Open Data Kit.
Agricultural support experts annotated the farm-labeled fecal images.
Then we used a baseline CNN model, VGG16, InceptionV3, MobileNetV2, and
Xception models. We trained models using farm and laboratory-labeled
fecal images and then fine-tuned them. The test set used farm-labeled
images. The test accuracies results without fine-tuning were 83.06\% for
the baseline CNN, 85.85\% for VGG16, 94.79\% for InceptionV3, 87.46\%
for MobileNetV2, and 88.27\% for Xception. Finetuning while freezing the
batch normalization layer improved model accuracies, resulting in
95.01\% for VGG16, 95.45\% for InceptionV3, 98.02\% for MobileNetV2, and
98.24\% for Xception, with F1 scores for all classifiers above 75\% in
all four classes. Given the lighter weight of the trained MobileNetV2
and its better ability to generalize, we recommend deploying this model
for the early detection of poultry diseases at the farm level. There are
open questions about the deployment of the model at the farm level and
potential areas for further research.

\emph{\textbf{Siddharth Sharma-Mishra: Simulation-based approaches to
astrophysics dark matter searches}}

We are at the dawn of a data-rich era in astrophysics and cosmology,
with the capacity to extract useful scientific insights often limited by
our ability to efficiently model complex processes that give rise to the
data rather than the volume and nature of observations itself. I will
describe recent progress in applying mechanistic forward modeling
techniques to a range of astrophysical observations with the goal of
searching for signatures of new physics, in particular the nature of
dark matter. These leverage developments in machine learning-aided
inference, e.g. using simulation-based inference as well as
differentiable probabilistic programming, while encoding domain
knowledge, in order to maximize the scientific output of current as well
as future experiments.

\emph{\textbf{Maren Büttner: Single-cell transcriptomics}}

Cells are the fundamental units of life. Understanding cellular
processes is a basis for improving human health, disease diagnosis and
monitoring. The advent of single-cell transcriptomics (scRNA-seq) allows
characterizing the gene expression patterns of entire organs and
organisms at single cell resolution. The human genome encodes more than
30.000 genes, and high-throughput scRNA-seq methods create samples with
tens of thousands of cell measurements. The analysis of such data
requires a variety of methods from the machine learning field, e.g.
dimensionality reduction techniques from PCA to variational
autoencoders, graph-based clustering, classification of cell types,
trajectory inference and causal inference of gene regulation to
understand cell fate decision making. To date, scRNA-seq is a widely
applied research technique, which has the potential for standard
application in the clinics.~ My presentation focusses on current
approaches for large-scale scRNA-seq data, current open questions, and
implications for human health.~~~~~~

\emph{\textbf{Christian Igel: Estimating ecosystem properties: Combining
machine learning and mechanistic models}}

Progress in remote sensing technology and machine learning algorithms
enables scaling up the monitoring of ecosystems. This leads to new
knowledge about their status and dynamics, which will be helpful in land
degradation assessment (e.g., deforestation), in mitigating poverty
(e.g., food security, agroforestry, wood products), and in managing
climate change (e.g., carbon sequestration). We apply deep learning for
the mapping of individual trees and forests. Tree crowns are segmented
in satellite imagery using fully convolutional neural networks. This
provides detailed measurements of the canopy area and of the
distribution of trees within and outside forests. Allometric equations
are applied to estimate the biomasses (and thereby the stored carbon) of
the individual trees. We use iterative gradient-based optimization of
the allometric models and suggest techniques such as jackknife+ for
quantifying the uncertainty of the model predictions. Tree biomass can
also be directly inferred from LiDAR (laser imaging, detection, and
ranging) measurements using 3D point cloud neural networks. This leads
to highly accurate results without requiring a digital elevation
model.~In a new project, we consider risk assessment of vector-borne
diseases based on deep learning and remote sensing. Malaria risk is
related to the housing conditions, for example, the type of roofing
material, which can be determined from satellite images.

~

\emph{\textbf{Ieva Kazlauskaite: Partial differential equations and
Variational Bayes}}

Inverse problems involving partial differential equations (PDEs) are
widely used in science and engineering. Although such problems are
generally ill-posed, different regularisation approaches have been
developed to ameliorate this problem. Among them is the Bayesian
formulation, where a prior probability measure is placed on the quantity
of interest. The resulting posterior probability measure is usually
analytically intractable. The Markov Chain Monte Carlo (MCMC) method has
been the go-to method for sampling from those posterior measures. MCMC
is computationally infeasible for large-scale problems that arise in
engineering practice. Lately, Variational Bayes (VB) has been recognised
as a more computationally tractable method for Bayesian inference,
approximating a Bayesian posterior distribution with a simpler trial
distribution by solving an optimisation problem. The talk covered some
recent experiences of applying Bayesian inference, generative models and
probabilistic programming languages in the context of learning material
properties in civil engineering and in ice sheet and ice core modelling.
The main shortcomings of PPLs and differentiable problems were
highlighted.~

\emph{\textbf{Francisco Vargas: The Schrödinger bridge problem}}

Recent works in diffusion-based models have been achieving competitive
results across generative modelling and inference, in this presentation
we propose to explore a unifying framework based on Schrodinger bridges
to explore/explain diffusion-based methodology. The Schrödinger bridge
problem (SBP) finds the most likely stochastic evolution between two
probability distributions given a prior (reference) stochastic
evolution. Recently SBP based methodology has made its way into
generative modelling , sampling, and inference. In this talk we propose
the exploration of a unifying framework for the aforementioned works
based on the renowned IPF/Sinkhorn algorithm. The motivation behind this
is to cast a unifying lens via the Schrodinger perspective relating
inference, sampling and transport, in a way that we can leverage many of
the useful techniques and heuristics from each field to benefit each
other.

\section{Building effective
simulations}\label{building-effective-simulations}

\subsection{Moving upstream}\label{moving-upstream}

Science proceeds through hypothesis, observation and analysis. For
hundreds of years, researchers have advanced the frontiers of knowledge
by collecting data, compressing those observations into a model, then
computing that model to create representations of how the world works,
generating new insights about natural and physical phenomena and
theories about the systems from which those phenomena emerge in the
process \citep{Blei-build2014}.
These mathematical models rely on numerical methods: algorithms that
help solve mathematical problems where no analytical solution is
available. Today, data collection and the basic computational tasks
involved in its analysis -- linear algebra, optimisation, simulation,
and so on -- remain consistent features of the scientific process.
Progress in machine learning, however, has changed the modelling
landscape.

`AI for science' offers a data-centric approach to modelling and
simulating the world. Operating alongside the traditional mathematical
models that are central to many disciplines, machine learning provides
data-centric analytical methods that can be integrated across the
scientific pipeline, for example enabling sophisticated simulations of
real-world systems. These simulations can be used to inform model
development, test hypotheses and shape areas of research focus, or
unlock insights from complex data.

\subsection{Nurturing a diversity of
approaches}\label{nurturing-a-diversity-of-approaches}

Simulations are a well-established tool for scientific discovery. Their
fundamental task is to allow data sampling from a model where the
differences between simulation and the real world are reduced as far as
feasible, to enable experimentation or testing of the impact of
different perturbations, while allowing some measure of simplification
of the system. Effective simulators allow researchers to move from
theory to an understanding of what data should look like.

Domains such as particle physics, protein folding, climate science, and
others, have developed complex simulations that use known theories and
parameters of interest to make predictions about the system of study. AI
for science can be brought in to speed up some of these through
surrogate models. Machine learning can complement `traditional'
approaches to scientific simulation, adding components that model the
most uncertain elements of a system to strongly mechanistic models that
might otherwise be too restrictive in their assumptions.

Much early excitement surrounding AI for science was rooted in the
reverse process, asking: instead of starting with theory, could
researchers instead start with the large amounts of data available in
many areas of research and, from that data, build an understanding of
what an underpinning theory might be? Given a set of observations, is it
possible to find parameters for a model that result in simulations that
reflect the measured data? Such simulation-based inference (SBI) offers
the opportunity to generate novel insights across scientific
disciplines.

To enable such analysis, machine learning methods are needed that can
extract insights from high-dimensional, multi-modal data, in ways that
are labour- and compute-efficient \cite{Cranmer-frontier2020}. The field of
probabilistic numerics offers a way to flexibly combine information from
mechanistic models with insights from data, solving numerical problems
through statistical approaches \cite{Hennig-probabilistic2022}. Operationalising these methods
to create effective data-driven simulations requires balancing different
model characteristics. The model's parameters must be specified to a
sufficient level of granularity to describe the real-world system, while
operating at a level of abstraction that is amenable to analysis and
computation; almost all models are `wrong' or falsifiable because of
this, but some level of abstraction is necessary to make them useful for
analysis. The simulation must also be designed to be robust, and able to
generate inferences that align with real-world observations.

\subsection{Truth, truthiness, and interfacing with the real
world}\label{truth-truthiness-and-interfacing-with-the-real-world}

The excitement underpinning AI for science stems from the aspiration to
unearth new understandings of the world, leveraging data to advance the
frontiers of knowledge. While subject to their own limitations, the
scientific community has developed checks and balances to scrutinise new
knowledge and maintain the rigour of scientific inquiry. Recent years
have seen a variety of challenges or benchmarks emerge in the machine
learning community that have come to represent the field's expected
standards of performance from algorithms on defined tasks. However,
these standards do not necessarily align with the expectations of domain
researchers.\footnote{Hermans, J., Delaunoy, A., Rozet, F., Wehenkel, A.
  and Louppe, G. (2021), Averting A Crisis In Simulation-Based
  Inference, arXiv:2110.06581 {[}stat.ML{]},
  \href{https://doi.org/10.48550/arXiv.2110.06581}{\uline{https://doi.org/10.48550/arXiv.2110.06581}}}
As data-centric simulations are integrated into scientific process,
machine learning researchers must consider their responsibility in
maintaining the integrity of the domains into which they are deployed,
raising the question: what guardrails are needed to ensure researchers
can be confident in the outputs from machine learning-enabled
simulations?

A variety of diagnostic tests can help. Core to many of these
diagnostics is analysis of whether a model is computationally faithful.
In short: the inferences generated by a simulation should reflect those
from observations.\footnote{Ibid.} One approach to checking this
alignment is to consider the consistency of distributions from inferred
and observed datasets. If the model is a good fit, the data it generates
should broadly match the data observed through experimentation.

Underpinning these diagnostics is a fundamental question about how to
manage uncertainty, in a context where different failure modes have
different implications. Put simply: when a model fails, is it worse to
be over-confident in its results, or over-conservative? In the
scientific context, over-confidence seems more likely to result in
negative outcomes, whether through giving misleading interpretations or
results or driving lines of enquiry in unproductive directions. Machine
learning methods can be designed for conservatism, reducing the risk of
false positives.

Implementing a schedule of model building, computing, critiquing, and
repeating can refine this process. One lesson from experiences of
building machine learning-enabled simulations is that there can be a
disconnect between how machine learning approaches inference and model
building, and how the same task is approached by domain scientists. From
a domain perspective, model building seems naturally an iterative
process: collect data, fit a model, find errors or areas for
improvement, update the model, and so on. This iterative process is
guided by expert intuition and knowledge; deep understanding of the
system under study and how it responds to perturbation. Machine learning
research has developed practices for prior elicitation -- using domain
knowledge to shape the structure of probabilistic models -- but the
nuances of this domain intuition are often not easily captured a priori,
instead emerging when models fail as an informal sense of what `feels'
like it should be true. This qualitative input is vital in building
effective simulations. It requires close collaboration, which in turn
requires an investment of time and energy from domain communities,
generated through mutual trust, incentives, and long-term
relationship-building.

\subsection{Connecting simulation to
practice}\label{connecting-simulation-to-practice}

Computational tools are central to the effective deployment of machine
learning-enabled simulation. The function and form of such tools must
align with the requirements of the community deploying them. Designing
computational systems to match user needs -- and work effectively in
practice -- requires both effective software engineering and close
collaboration with domain groups that can articulate the requirements
and expectations of those working in the field. To remain effective over
the longer-term, such systems must leverage effective software
engineering practices, including embedding version control and building
interfaces that work with other models and systems. Those practices, and
the software systems that emerge from them, must be designed for the
needs of those using the system, drawing from existing best practices in
software engineering, but adapting those practices to reflect the needs
of the domain for deployment.

Constructing computational tools requires a mix of technical insight and
craft skill -- of knowledge and know how. Tools produced by the machine
learning community differ in their usefulness on different problems:
some work well for certain tasks, but not for others. Without access to
such craft skills, those outside the `AI for science' community can find
it challenging to determine which tools to use for which purposes,
reducing the generalisability of existing methods and approaches. This
challenge becomes particularly visible when practitioners are tightly
integrated into the analysis pipeline, such as in applications in
developmental biology, in the developing world, and in data-centric
engineering. Widening access to the field will require user guides that
characterise which simulations are effective for which tasks or
purposes, supported by case studies or user stories that help demystify
how machine learning can work in practice.
