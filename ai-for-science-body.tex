\textbf{AI for science: an emerging agenda}

\emph{Working Paper from: Bridging Data-driven and Mechanistic Modelling
{[}Dagstuhl 22382{]}}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\include{summary.tex}
\hypertarget{contents}{%
\section{Contents}\label{contents}}

\protect\hyperlink{summary}{Summary \protect\hyperlink{summary}{1}}

\protect\hyperlink{_Toc121762019}{Contents
\protect\hyperlink{_Toc121762019}{3}}

\protect\hyperlink{introduction-bridging-data-driven-and-mechanistic-modelling}{Introduction:
bridging data driven and mechanistic modelling
\protect\hyperlink{introduction-bridging-data-driven-and-mechanistic-modelling}{4}}

\protect\hyperlink{snapshots-of-ai-in-science}{Snapshots of AI in
science \protect\hyperlink{snapshots-of-ai-in-science}{6}}

\protect\hyperlink{in-earth-sciences}{In Earth Sciences
\protect\hyperlink{in-earth-sciences}{6}}

\protect\hyperlink{in-environmental-and-agricultural-sciences}{In
environmental and agricultural sciences
\protect\hyperlink{in-environmental-and-agricultural-sciences}{8}}

\protect\hyperlink{in-physical-sciences}{In physical sciences 9}

\protect\hyperlink{section}{In biological sciences 11}

\protect\hyperlink{building-effective-simulations}{Building effective
simulations 15}

\protect\hyperlink{moving-upstream}{Moving upstream 15}

\protect\hyperlink{nurturing-a-diversity-of-approaches}{Nurturing a
diversity of approaches 15}

\protect\hyperlink{truth-truthiness-and-interfacing-with-the-real-world}{Truth,
truthiness, and interfacing with the real world 16}

\protect\hyperlink{connecting-simulation-to-practice}{Connecting
simulation to practice 16}

\protect\hyperlink{directions}{Directions 17}

\protect\hyperlink{connecting-data-to-causality}{Connecting data to
causality 20}

\protect\hyperlink{causality-in-science-and-data}{Causality in science
and data 20}

\protect\hyperlink{causal-models-as-a-route-to-advancing-the-science-of-ai-and-ai-for-science}{Causal
models as a route to advancing the science of AI and AI for science 20}

\protect\hyperlink{from-methods-to-application}{From methods to
application 21}

\protect\hyperlink{directions-1}{Directions 22}

\protect\hyperlink{encoding-domain-knowledge}{Encoding domain knowledge
25}

\protect\hyperlink{wheres-my-science-jetpack}{Where's My {[}Science{]}
Jetpack? 25}

\protect\hyperlink{encoding-domain-knowledge-through-model-design}{Encoding
domain knowledge through model design 25}

\protect\hyperlink{scientific-centaurs}{Scientific centaurs 26}

\protect\hyperlink{enabling-communication-across-domains}{Enabling
communication across domains 27}

\protect\hyperlink{directions-2}{Directions 28}

\protect\hyperlink{a-research-agenda-in-ai-for-science}{A research
agenda in AI for science 32}

\protect\hyperlink{accelerating-progress-in-ai-for-science}{Accelerating
progress in AI for science 34}

\protect\hyperlink{advance-new-methods-and-applications}{Advance new
methods and applications 34}

\protect\hyperlink{invest-in-tools-and-toolkits}{Invest in tools and
toolkits 35}

\protect\hyperlink{build-capability-across-disciplines}{Build capability
across disciplines 35}

\protect\hyperlink{grow-communities-of-research-and-practice}{Grow
communities of research and practice 36}

\protect\hyperlink{ai-and-science-building-the-interface}{AI and
science: building the interface 37}

\protect\hyperlink{annex-1-participants-at-machine-learning-for-science-bridging-data-driven-and-mechanistic-modelling-dagstuhl-seminar-22382-18-23-september-2022}{Annex
1: Participants at Machine Learning for Science: Bridging Data-driven
and Mechanistic Modelling (Dagstuhl Seminar 22382, 18-23 September 2022)
39}

\protect\hyperlink{_Toc121762051}{Annex 2: Research questions arising
from the `AI for science research agenda' discussion during the Dagstuhl
workshop \protect\hyperlink{_Toc121762051}{40}}

\hypertarget{introduction-bridging-data-driven-and-mechanistic-modelling}{%

\include{introduction-bridging-data-driven-and-mechanistic-modelling}

\include{snapshots-of-ai-in-science}{%

\hypertarget{directions}{%
\subsection{Directions}\label{directions}}

Machine learning typically requires an explicit representation of a
likelihood, but these are often difficult to compute. Further advances
in SBI are necessary to allow researchers to identify model parameters
from data.

\begin{itemize}
\item
  Techniques such as likelihood-free inference can enhance existing
  Bayesian methods for inferring posterior estimations.\footnote{Alsing,
    J., Charnock, T., Feeney, S. and Wandelt, B. (2019) Fast
    likelihood-free cosmology with neural density estimators and active
    learning, arXiv:1903.00007 {[}astro-ph.CO{]},

    \uline{\url{https://doi.org/10.48550/arXiv.1903.00007}}}
\item
  Building surrogate models,\footnote{See above, and Lavin, A., Zenil,
    H., Paige, B., Krakauer, D., Gottschlich, J., Mattson, T.,
    Anandkumer, A., Choudry, S., Rocki, K., Baydin, A.G., Prunkl, C.,
    Paige, B., Isayev, O., Peterson, E., McMahon, P.L., Macke, J.,
    Cranmer, K., Zhang, J., Wainwright, H., Hanuka, A., Veloso, M.,
    Assefra, S., Zheng, S., and Pfeffer, A. (2021) Simulation
    Intelligence: Towards a New Generation of Scientific Methods,
    arXiv:2112.03235 {[}cs.AI{]},
    \href{https://doi.org/10.48550/arXiv.2112.03235}{\hfill\break
    \uline{https://doi.org/10.48550/arXiv.2112.03235}}} using Bayesian
  approaches for simulation planning to optimise information
  gain,\footnote{See, for example, Cranmer, K., Heinrich, L., Head, T.
    and Louppe, G. Active sciencing, at:
    \href{https://github.com/cranmer/active_sciencing}{\uline{https://github.com/cranmer/active\_sciencing}}}
  or deploying emulations\footnote{Boelts, J., Lueckmann, J.M., Gao, R.,
    and Macke, J.H. (2022) Flexible and efficient simulation-based
    inference for models of decision-making eLife 11:e77220
    \href{https://doi.org/10.7554/eLife.77220}{\uline{https://doi.org/10.7554/eLife.77220}}}
  can also enhance the efficiency of simulations.
\item
  Probabilistic numerics offers a route to develop statistically-optimal
  algorithms that are amenable to comprehensive uncertainty
  quantification, leveraging Gaussian Process-based Ordinary
  Differential Equation (ODE) solvers to pursue simulation as an
  inference problem.\footnote{Kersting, H. (2021) Uncertainty-aware
    numerical solutions of ODE's by Bayesian Filtering, available at:
    \href{https://hanskersting.github.io/publication/phd-thesis/}{\uline{https://hanskersting.github.io/publication/phd-thesis/}}}
\end{itemize}

Operationalising these approaches will also require new toolkits to
support implementation of probabilistic numerical methods.\footnote{See,
  for example, the previous Dagstuhl meeting on this topic:
  \href{https://www.probabilistic-numerics.org/meetings/2021_Dagstuhl/}{\uline{https://www.probabilistic-numerics.org/meetings/2021\_Dagstuhl/}}
  and Schmidt, J., Kramer, N. and Hennig, P. (2021) A Probabilistic
  State Space Model for Joint Inference from Differential Equations and
  Data, arXiv:2103.10153 {[}stat.ML{]},
  \href{https://doi.org/10.48550/arXiv.2103.10153}{\uline{https://doi.org/10.48550/arXiv.2103.10153}}}

Computational faithfulness -- alignment of inferred parameters with
scientific knowledge -- can be achieved through:

\begin{itemize}
\item
  Diagnostic checks in the self-consistency of the Bayesian joint
  distribution, which measure the scientific quality of the regions
  computed by Bayesian SBI methods.\footnote{Hermans, J., Delaunoy, A.,
    Rozet, F., Wehenkel, A. and Louppe, G. (2021), Averting A Crisis In
    Simulation-Based Inference, arXiv:2110.06581 {[}stat.ML{]},
    \href{https://doi.org/10.48550/arXiv.2110.06581}{\uline{https://doi.org/10.48550/arXiv.2110.06581}}
    and Mishra-Sharma, S. (2021) Inferring dark matter substructure with
    astrometric lensing beyond the power spectrum, arXiv:2110.01620
    {[}astro-ph.CO{]},
    \href{https://doi.org/10.48550/arXiv.2110.01620}{\uline{https://doi.org/10.48550/arXiv.2110.01620}}}
  Checking for self-consistency gives a sense whether the model is `good
  enough' (ie whether the inference engine gives a good sense of the
  posterior).
\item
  Enforcing conservative neural ratio estimation through binary
  classifier specification, producing more conservative posterior
  approximations.\footnote{Delaunoy, A., Hermans, J., Rozet, F.,
    Wehenkel, A. and Louppe, G. (2022) Towards Reliable Simulation-Based
    Inference with Balanced Neural Ratio Estimation, arXiv:2208.13624
    {[}stat.ML{]},
    \href{https://doi.org/10.48550/arXiv.2208.13624}{\uline{https://doi.org/10.48550/arXiv.2208.13624}}}
\item
  Hybrid modelling, which combines machine learning components learned
  from data with the mechanistic components specified by existing domain
  knowledge.\footnote{Wehenkel, A., Behrmann, J., Hsu, H., Sapiro, G.,
    Louppe, G., and Jacobsen, J.H. (2022) Robust Hybrid Learning With
    Expert Augmentation, arXiv:2202.03881 {[}cs.LG{]},
    https://doi.org/10.48550/arXiv.2202.03881}
\item
  Further study of the impact of model misspecification could also help
  generate new robustness diagnostic checks.\footnote{\href{https://arxiv.org/abs/2209.01845}{Cannon,}
    P., Ward, D. and Schmon, S.M. (2022) Investigating the Impact of
    Model Misspecification in Neural Simulation-based Inference,
    arXiv:2209.01845 {[}stat.ML{]},
    \uline{https://doi.org/10.48550/arXiv.2209.01845}}
\end{itemize}

`Digital twins' have recently received much attention as a tool to
exploit sophisticated simulations. In Earth sciences, for example,
ambitious efforts to develop a digital twin of the Earth propose to
allow more accurate forecasting, visualisation, or scenario-testing of
the impact of climate change and efforts to mitigate it.\footnote{For
  example: European Commission (2022) Destination Earth -- new digital
  twin of the Earth will help tackle climate change and protect nature,
  available at:
  https://ec.europa.eu/commission/presscorner/detail/en/IP\_22\_1977}
The challenge is to integrate different models or components of a system
-- for example, connecting atmospheric models, with land models, with
models of human behaviour -- in a way that represents the complete Earth
system. That requires consideration of the different levels of
granularity with which these different models operate: economic models
of human behaviour, for example, operate with different assumptions and
levels of enquiry in comparison to physical models of ocean circulation.
The full range of granularities becomes apparent when considering that
specific applications, such as disease monitoring on poultry farms, sit
within the wider ecosystem of the natural and built environment. A
digital twin needs to make choices about what levels of granularity it
is operating at, from the scale of the poultry farm to the planet. The
questions that emerge from such ambitions is: what level of granularity
is helpful or necessary to deliver effective results? And what
interfaces between diverse models might be possible?

\textbf{Box 2: Talks given during this workshop session}

\emph{\textbf{Philipp Hennig: Information from data and compute in
scientific inference}}

Simulations are central to scientific inference. Simulators are
typically treated as black boxes, with the inference loop wrapped around
them. This approach is convenient for the programming scientists, but
can be highly inefficient. Probabilistic numerical methods represent
computational and empirical data in the same language, which allows for
inference from mechanistic knowledge and empirical data in one combined
step. I will argue that scientific computing needs to embrace such new
computational paradigms to truly leverage ML in science, which also
requires rethinking scientific codebases.

\emph{\textbf{Hans Kersting: ODE filters and smoothers: probabilistic
numerics for mechanistic modelling}}

Probabilistic numerics (PN) unifies statistical and numerical
approximations by formulating them in the same language of statistical
(Bayesian) inference. For ODEs, a well-established probabilistic
numerical method is ODE filters and smoothers which can help to deal
more aptly with uncertainty in mechanistic modeling. In the first half
of this talk, we will first introduce PN and then present ODE
filters/smoothers as a specific instance of PN. In the second half, we
will discuss how ODE filters/smoothers can improve mechanistic modeling
in the natural sciences and present a recent application of inferring
the parameters of real-word dynamical system.

\emph{\textbf{Jakob Macke: Four short stories on simulation-based
inference}}

Many fields of science make extensive use of simulations expressing
mechanistic forward models, requiring the use of simulation-based
inference methods. I will share experiences and lessons learned from
four applications: Describing the dynamics and energy consumptions of
neural networks in the stomatogastric ganglion; inferring parameters of
gravitational wave models; optimising single-molecule localisation
microscopy, and building computational models of the fly visual system.
I will try to convey some thoughts on the challenges and shortcomings of
current approaches.~

\emph{\textbf{Gilles Louppe: Towards reliable simulation-based inference
and beyond}}

Modern approaches for simulation-based inference build upon deep
learning surrogates to enable approximate Bayesian inference with
computer simulators. In practice, the estimated
posteriors\textquotesingle{} computational faithfulness is, however,
rarely guaranteed. For example, Hermans et al., 2021 have shown that
current simulation-based inference algorithms can produce posteriors
that are overconfident, hence risking false inferences. In this talk, we
will review the main inference algorithms and present Balanced Neural
Ratio Estimation (BNRE), a variation of the NRE algorithm designed to
produce posterior approximations that tend to be more conservative,
hence improving their reliability.

\emph{\textbf{Tom Dietterich: Modeling the data collection process: My
journey}}

In this talk, I will describe three examples of my attempts to integrate
subject-matter knowledge with machine learning. The first example
involves predicting grasshopper infestations. I will sketch the
methodology in which we first modeled the life cycle of the grasshoppers
to capture the factors that affect their population. Unfortunately, most
variables of interest were not measured, so we used the model to guide
the construction of proxy variables. Ultimately, this project did not
succeed, but it is hard to determine whether this is due to modeling
problems or to the chaotic nature of the biological phenomenon.

\hypertarget{connecting-data-to-causality}{%
\section{Connecting data to
causality}\label{connecting-data-to-causality}}

\hypertarget{causality-in-science-and-data}{%
\subsection{Causality in science and
data}\label{causality-in-science-and-data}}

Most scientific endeavours have a causal element: researchers want to
characterise how a system works, why it works that way, and what happens
when it is perturbed. How researchers identify cause-and-effect
relationships varies across domains. For some disciplines, the process
of hypothesis design -- data collection -- model development provides
the core structure for interrogating how a system works. In others,
where experimentation is more difficult, researchers may rely on natural
experiments and observations to compare the response of a system under
different conditions. Those studying the Earth system, for example, have
little scope to replicate planetary conditions, so instead rely on
observational data and modelling to identify the impact of different
interventions. These different approaches, however, share a modelling
approach in which researchers provide variables to create structural,
causal models.

In contrast, machine learning proceeds by learning representations or
rules from data, based on statistical information, rather than
structured rules about how a system works (such as physical laws).
Causal inference -- the ability to identify cause-and-effect
relationships in data -- has been a core aim of AI research, in service
of both wider ambitions to replicate intelligence in machines and
efforts to create AI systems that are robust in deployment. However, in
many respects efforts to integrate causal inference into AI systems have
yet to deliver.\footnote{Schölkopf, B., Locatello, F., Bauer, S., Ke,
  N.R., Kalchbrenner, N., Goyal, A., and Bengio, Y. (2021) Towards
  Causal Representation Learning,
  \href{https://arxiv.org/abs/2102.11107}{\uline{arXiv:2102.11107}}
  {[}cs.LG{]},
  \href{https://doi.org/10.48550/arXiv.2102.11107}{\uline{https://doi.org/10.48550/arXiv.2102.11107}}}

An apocryphal story in AI tells of efforts by US researchers during the
1980s to train a computer system that could distinguish between images
of tanks from the US and USSR. The resulting system delivered high
accuracy on its training data, but failed repeatedly in practice. The
system was subsequently found to be classifying images based on their
resolution and background features -- is the image grainy? Does it
contain snow? -- rather than the tanks themselves. It found patterns in
the data that were co-incident, rather than causal. That same error has
real-world implications for the AI systems deployed today. In medical
sciences, AI systems trained to detect collapsed lungs from medical
images have been proven inaccurate, after the model was found to have
learned to detect the tube inserted into the lung to enable a patient to
breath as a response to its collapse, rather than the physical features
of the lung itself.\footnote{Rueckel, J., Trappmann, L., Schachtner, B.,
  Wesp, P., Hoppe, B.F., Fink, N., Ricke, J., Dinkel, J., Ingrisch, M.,
  and Sabel, B.O. (2020) Impact of Confounding Thoracic Tubes and
  Pleural Dehiscence Extent on Artificial Intelligence Pneumothorax
  Detection in Chest Radiographs. Invest Radiol. 2020
  Dec;55(12):792-798. doi: 10.1097/RLI.0000000000000707. PMID: 32694453.
  \href{https://pubmed.ncbi.nlm.nih.gov/32694453/}{\uline{https://pubmed.ncbi.nlm.nih.gov/32694453/}}}
In medical sciences, deployment of such systems could put patient care
at risk. In social sciences, these AI design and data bias failures can
combine to marginalise vulnerable populations.\footnote{Emspak, J.
  (2016) How a machine learns prejudice, Scientific American Blogs,
  available at:
  \href{https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/}{\uline{https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/}}}

Conversely, an understanding of the structures within data can improve
the accuracy of machine learning analyses. In exoplanet discovery, for
example, machine learning is used as a tool to detect variations in
light signals from large-scale astronomical datasets. The movement of
exoplanets around stars results in periodic changes to the light signals
from those stars, as the planet obscures them in its transit. Machine
learning can detect those signals and predict where exoplanets might be
located, but the data is often noisy. Noticing that the structure of
this noise was consistent across a number of stars, which were too
distant from each other to be interacting, researchers concluded that
instrumentation effects were distorting the data, and developed a method
to model those effects and remove them from exoplanet predictions. The
result was an efficient method for exoplanet identification that
subsequently contributed to the discovery of the first potentially
habitable planet.\footnote{Schölkopf, B. (2019) Causality for machine
  learning, arXiv:1911.10500 {[}cs.LG{]},
  \href{https://doi.org/10.1145/3501714.3501755}{\uline{https://doi.org/10.1145/3501714.3501755}}}

\hypertarget{causal-models-as-a-route-to-advancing-the-science-of-ai-and-ai-for-science}{%
\subsection{Causal models as a route to advancing the science of AI and
AI for
science}\label{causal-models-as-a-route-to-advancing-the-science-of-ai-and-ai-for-science}}

Many of these errors in misdiagnosing cause-effect relationships arise
from a core assumption in many machine learning methods: that data
follows an independent and identical distribution (IID). In practice,
almost all data from real-world, or complex, systems will violate this
assumption, given the interconnectedness of different variables. The
task of causality in machine learning is to create models that can
manage this violation, distinguishing between patterns in data that
simply co-occur and patterns that are causal. The resulting AI systems
would be able to solve a task in many different environments, based on
an understanding of the fundamental causal mechanisms in a
system.\footnote{Peters, J., Janzing, D. and Schölkopf, B. (2017)
  Elements of causal inference: foundations and learning algorithms, MIT
  Press, Cambridge, MA, ISBN 9780262037310} They would be more robust in
deployment, being less likely to make incorrect predictions as the
environment in which they operate changes, and could be more efficient
to train and deploy. They would also represent a step towards
replicating human- or animal-like intelligence, being able to solve a
task in many different environments.

In these regards, causal machine learning offers a route to balancing
the widespread utility of statistical modelling with the strengths of
physical models. Causality allows models to operate at a level of
abstraction beyond strongly mechanistic approaches, such as those based
on differential equations, moving along a continuum from mechanistic to
data-driven modelling. They provide researchers with the ability to make
accurate predictions under conditions of dataset shift (enable out of
distribution generalisation); can provide insights into the physical
processes that drive the behaviour of a system; unlock progress towards
AI systems that `think' in the sense of acting in an imagined space;
while also leveraging insights that can be learned from data, but not
otherwise detected.\footnote{For reference, see the table on page 11 of
  reference 69.} They also offer opportunities to explore
counterfactuals in complex systems, asking what the impact of different
interventions could have been, opening a door to the development of
simulation-based decision-making tools.\footnote{Such tools may have
  particular relevance in policy. For example: Mastakouri, A.A. and
  Schölkopf, B. (2020) Causal analysis of Covid-19 spread in Germany,
  arXiv:2007.11896 {[}stat.AP{]},
  \href{https://doi.org/10.48550/arXiv.2007.11896}{\uline{https://doi.org/10.48550/arXiv.2007.11896}}}

Achieving this potential requires technical developments in a number of
directions, but can also yield more effective AI systems. Such systems
would:

\begin{itemize}
\item
  Be able to operate on out of distribution data, performing the task
  for which they are trained in environments with varying conditions.
\item
  Be able to learn how to perform a task based on relatively few
  examples of that task in different conditions, or be able to rapidly
  adapt what they have learned for application in new environments
  through transfer, one-shot, or lifelong learning approaches.
\item
  Support users to analyse the impact of different interventions on a
  system, providing explanations or ways of attributing credit to
  different actions.
\item
  Respond to different ways of transmitting information between
  individuals and groups, enabling effective communication with their
  users or other forms of cultural learning.
\end{itemize}

\hypertarget{from-methods-to-application}{%
\subsection{From methods to
application}\label{from-methods-to-application}}

Achieving the level of technical sophistication required for causal
modelling requires careful model design, based on close collaboration
between machine learning and domain scientist. The process of specifying
what to represent in a causal machine learning system involves a series
of `micro-decisions' about how to construct the model, negotiated by
integrating machine learning and domain expertise. In this regard,
causal machine learning can be a positive catalyst for deeper
interdisciplinary collaboration; model construction can be a convening
point for sharing understandings between domains. However, the level of
detail required can also be in tension with efforts to promote
widespread adoption of AI methods across research. The availability of
easy-to-use, off-the-shelf AI tools has been an enabler for adoption in
many domains. The hand-crafted approach inherent to current causal
methods renders them less accessible to non-expert users. Part of the
challenge for the field is to make such methods more broadly accessible
through open-source toolkits or effective software engineering
practices.

This tension between specification and learning also highlights the
importance of nurturing a diversity of methods across the spectrum from
data-driven to mechanistic modelling. The domain (or, how much prior
knowledge is available and what knowledge should be included), research
question of interest, and other practical factors (including, for
example, compute budget), will shape where along this spectrum
researchers wish to target their modelling efforts.

While pursuing practical applications, advances in causal inference
could help answer broader questions about the nature of intelligence and
the role of causal representations in human understanding of how the
worlds work. Much of human understanding of the world arises from
observing cause and effect; seeing what reaction follows an intervention
-- that an object falls when dropped, for example -- in a way that
generalises across circumstances and does not require detailed
understanding of mathematical or physical laws. Integrating this ability
into machine learning would help create systems that could be deployed
on a variety of tasks. The process of building causal machine learning
forces researchers to interrogate the nature of causal representations
-- What are they? How are they constructed from the interaction between
intelligent agents and the world? By what mechanism can such agents
connect low-level observations to high-level causal variables? -- which
may in turn support wider advances in the science of AI.

\hypertarget{directions-1}{%
\subsection{Directions}\label{directions-1}}

Causality in machine learning is a long-standing and complex challenge.
In the context of scientific discovery, learning strategy, model design,
and encoding domain knowledge all play a role in helping identify
cause-effect relationships.

Different learning strategies can improve the `generalisability' of
machine learning, increasing its performance on previously unseen tasks,
based on learning underlying structure of a task or environment in ways
that can contribute to broader understandings of causality. Such
learning strategies include:

\begin{itemize}
\item
  Transfer learning, taking learning from one task or domain and
  applying it in another.
\item
  Multi-task learning, enabling a system to solve multiple tasks in
  multiple environments.
\item
  Adversarial learning, to reduce the vulnerability of models to
  performance degradation on out-of-distribution data.
\item
  Causal representation learning, defining variables that are related by
  causal models.\footnote{Schölkopf, B. (2019) Causality for machine
    learning, arXiv:1911.10500 {[}cs.LG{]},
    \href{https://doi.org/10.1145/3501714.3501755}{\uline{https://doi.org/10.1145/3501714.3501755}}}
\item
  Reinforcement learning strategies that reward agents for identifying
  policies based on invariances over different conditions.
\end{itemize}

Across these new learning approaches, attempts to establish causal
mechanisms are also prompting progress in machine learning theory,
through statistical formulations of core principles.\footnote{Guo, S.,
  Tóth, V., Schölkopf, B. and Huszár, F. (2022) Causal de Finetti: On
  the Identification of Invariant Causal Structure in Exchangeable Data,
  arXiv:2203.15756 {[}stat.ML{]},
  https://doi.org/10.48550/arXiv.2203.15756}

Combining different methods can also enhance the functionality of an AI
system. For example:

\begin{itemize}
\item
  Neural ODEs has been shown to identify causal structures in time
  series data.\footnote{Aliee, H., Theis, F. J. and Kilbertus, N. (2021)
    Beyond Predictions in Neural ODEs: Identification and Interventions,
    arXiv:2106.12430 {[}cs.LG{]},
    \href{https://doi.org/10.48550/arXiv.2106.12430}{\uline{https://doi.org/10.48550/arXiv.2106.12430}}}
\item
  Describing causal effects as objective functions in constrained
  optimisation problems can deliver a form of stochastic causal
  programming.\footnote{Padh, K., Zeitler, J., Watson, D., Kusner, M.,
    Silva, R., and Kilbertus, N. (2022) Stochastic Causal Programming
    for Bounding Treatment Effects, arXiv:2202.10806 {[}stat.ML{]},
    \url{https://doi.org/10.48550/arXiv.2202.10806}}
\item
  Technical interventions\footnote{\href{https://arxiv.org/abs/2005.03353}{Jakobsen,}
    M. E., and Peters, J. (2020) Distributional robustness of K-class
    estimators and the PULSE, arXiv:2005.03353 {[}econ.EM{]},
    \href{https://doi.org/10.48550/arXiv.2005.03353}{\uline{https://doi.org/10.48550/arXiv.2005.03353}}}
  can constrain or optimise a model towards causal outcomes. As with
  simulation design, diagnostic checks can also help identify
  cause-effect relationships by examining model outputs against `reality
  criteria',\footnote{Including syntactic, semantic, and pragmatic
    elements: Stadler, M. \& Kruse, P. (1990) Über Wirklichkeitskrite-
    rien. In: Riegas, V. \& Vetter, C. (eds.) Zur Biologie der
    Kognition. Frankfurt a. M.: Suhrkamp} which compare outputs to
  real-world results.
\end{itemize}

There are also a variety of approaches to representing existing
scientific knowledge in machine learning models, notably by specifying
the assumptions made about the world through symmetries, invariances,
and physical laws (see Figure 1).

\textbf{Box 3: Talks given during this workshop session}

\emph{\textbf{Bernhard Schölkopf: Causality, causal digital twins, and
their applications}}

1.Desiderata for causal machine learning: work with (and benefit from)
non-IID data, multi-task/multi-environment, sample-efficient, OOD,
generalisation from observation of marginals, interventional.

2.Modelling taxonomy: differential equations, causal models, statistical
models.

3.How to get from one level to the next.

4.How to transfer between statistical models that share the same
underlying causal model.~

5.The assumption of independent causal mechanisms (ICM) (for example,
invariance/autonomy) and sparse mechanism design.~

6.How to derive the arrow of time from ICM and algorithmic information
theory.

7.Statistical formulation of ICM: causal de Finetti.

8.Application to exoplanet discovery and Covid-19 vaccine scenarios.

9.Causal representations as (a) causal digital twins and (b) AI models.

\emph{\textbf{Jonas Peters: Invariance: From Causality to Distribution
Generalization}}

Assume that we observe data from a response Y and a set of covariates X
under different experimental conditions (or environments). Rather than
focusing on the model that is most predictive, it has been suggested to
take into account the invariance of a model. This can help us to infer
causal structure (Which covariates are causes of Y?) and find models
that generalize better (How well does the model perform on an unseen
environment?). We show a few applications of these general principles
and discuss first steps towards understanding the corresponding
theoretical guarantees and limits.

\emph{\textbf{Niki Kilbertus: Can we discover dynamical laws from
observation?~}}

I will start with a brief introduction to identifiability of ODE systems
from a unique continuous or discrete observed solution trajectory. Then,
I will provide an overview of modern approaches to inferring dynamical
laws (in the form of ODEs) from observational data with a particular
focus on interpretability and symbolic methods. Finally, I will describe
our recent attempts and results at inferring scalar ODEs in symbolic
form from a single irregularly sampled, noisy solution trajectory.

\emph{\textbf{Soledad Villar: Invariances and equivariances in machine
learning}}

In this talk, we give an overview of the progress in the last few years
by several research groups in designing machine learning methods that
repeat physical laws. Some of these frameworks make use of irreducible
representations, some make use of high-order tensor objects, and some
apply symmetry enforcing constraints. Our work shows that it is simple
to parameterise universally approximating functions that are equivariant
under actions of the Euclidean, Lorentz, and Poincare group at any
dimensionality. The key observation is that O(d)-equivariant (and
related group-equivariant) functions can be universally expressed in
terms of a lightweight collection of dimensionless scalars (scalar
products and scalar contractions of the scalar, vector, and tensor
inputs). We complement our theory with numerical examples that show that
the scalar-based method is simple and efficient, and mention ongoing
work on cosmology simulations.~

\emph{\textbf{Bubacarr Bah: Divide-and-Conquer Equation Learning with R2
and Bayesian Model Evidence}}

Deep learning is a powerful method for tasks like predictions and
classification, but lacks interpretability and analytic access. Instead
of fitting up to millions of parameters, an intriguing alternative for a
wide range of problems would be to learn the governing equations from
data. Resulting models would be concise, parameters can be interpreted,
the model can adjust to shifts in data, and analytic analysis allows for
extra insights. Common challenges are model complexity identification,
stable feature selection, expressivity, computational feasibility, and
scarce data. In our work, the mentioned challenges are addressed by
combining existing methods in a novel way. We choose multiple regression
as a framework and argue how a surprisingly large space of model
equations can be captured. For feature selection, we exploit the
computationally cheap coefficient of determination
(R\textsuperscript{2}) to loop through millions of models, and by using
a divide-and-conquer strategy, we are able to rule out remaining models
in the equation class. Final model selection is achieved by exact values
of the Bayesian model evidence with empirical priors, which is known to
identify suitable model complexity without relying on mass data. Random
polynomials, and a couple of chaotic systems are used as examples.

\hypertarget{encoding-domain-knowledge}{%
\section{Encoding domain knowledge}\label{encoding-domain-knowledge}}

\hypertarget{wheres-my-science-jetpack}{%
\subsection{\texorpdfstring{Where's My {[}Science{]} Jetpack?
}{Where's My {[}Science{]} Jetpack? }}\label{wheres-my-science-jetpack}}

Humans have a long history of imagining futures where human progress is
accelerated by intelligent machines. Embedded in these visions for the
future are aspirations that AI can be a faithful servant, easing daily
activities or enhancing human activities.\footnote{The Royal Society
  (2018) AI narratives: portrayals and perceptions of artificial
  intelligence and why they matter, available at:
  \href{https://royalsociety.org/topics-policy/projects/ai-narratives/}{\uline{https://royalsociety.org/topics-policy/projects/ai-narratives/}}}
As with many emerging technologies, the reality of AI today looks
different to these Sci-Fi futures.\footnote{The title of this section is
  inspired by:
  \href{https://www.fantasticfiction.com/w/daniel-h-wilson/where-s-my-jetpack.htm}{\uline{https://www.fantasticfiction.com/w/daniel-h-wilson/where-s-my-jetpack.htm}}}
Practical experiences of deploying AI highlights a range of potential
failure modes, often rooted in insufficient contextual awareness,
misspecification of user needs, or misunderstanding of environmental
dynamics.\footnote{Paleyes, A., Urma, R.G. and Lawrence, N.D. (2022)
  Challenges in Deploying Machine Learning: A Survey of Case Studies,
  ACM Computing Surveys,
  \href{https://doi.org/10.1145/3533378}{\uline{https://doi.org/10.1145/3533378}}}

Today's science builds on thousands of years of attempts to understand
the world, which can be leveraged to design AI that serves scientific
goals. The result should be a collaborative endeavour between humans and
machines. Researchers need the analytical power of AI to make sense of
the world, while AI needs input from human understandings of the domain
in which it is deployed to function effectively; both need well-designed
human-machine interfaces to make this collaboration work. In this
context, effective integration of domain knowledge into AI systems is
vital, and three (broad) strategies have emerged to facilitate this
encoding: algorithmic design; AI integration in the lab; and effective
communication and collaboration.

\hypertarget{encoding-domain-knowledge-through-model-design}{%
\subsection{Encoding domain knowledge through model
design}\label{encoding-domain-knowledge-through-model-design}}

Traditional modelling approaches make use of well-defined rules or
equations that explain the dynamics of the system under study. The laws
of physics, for example, describe how energy moves through a system,
based on conservation principles. These laws are complemented by
mathematical symmetries that arise from our abstract representations of
physical objects and describe what features of an object remain
consistent, despite changes or transformations in a system.\footnote{Villar,
  S., Hogg, D.W., Storey-Fisher, K., Yao, W. and Blum-Smith, B. (2021)
  Scalars are universal: Equivariant machine learning, structured like
  classical physics, arXiv:2106.06610 {[}cs.LG{]},
  \href{https://doi.org/10.48550/arXiv.2106.06610}{\uline{https://doi.org/10.48550/arXiv.2106.06610}}
  and Villar, S., Yao, W., Hogg, D.W., Blum-Smith, B. and Dumitrascu, B.
  (2022) Dimensionless machine learning: Imposing exact units
  equivariance, arXiv:2204.00887 {[}stat.ML{]},
  \href{https://doi.org/10.48550/arXiv.2204.00887}{\uline{https://doi.org/10.48550/arXiv.2204.00887}}}
There may also be known invariances in a system: factors that do not
change under any perturbations or that change in a defined
way.\footnote{Ling, J., Jones, R. E., and Templeton, J.A. (2016) Machine
  learning strategies for systems with invariance properties. United
  States.
  \href{https://doi.org/10.1016/j.jcp.2016.05.003}{\uline{https://doi.org/10.1016/j.jcp.2016.05.003}}}
Building on this existing knowledge, and connecting to efforts to
generate causal understandings of the world through machine learning, an
area of growing interest has been the design of machine learning models
that respect these rules or symmetries.

The principle underpinning this design strategy is that it is possible
to move across a continuum from statistical (data-driven) models to
strongly mechanistic models, creating hybrid systems whose outputs
should be constrained by what is physically feasible, while also
leveraging insights from data (Figure 1).

\includegraphics[width=5.95833in,height=2.7581in]{media/image1.png}

At one end of that continuum, mechanistic models would obey known laws
or principles in a strongly deterministic way; at the other, statistical
models encode fewer assumptions and rely more on data. The addition of
invariances and symmetries, alongside other forms of domain knowledge,
allows bridging between these two model classes (Figure 2). Models that
describe how much heat is absorbed by the oceans under conditions of
climate change, for example, should obey the laws of thermodynamics and
energy conservation. By encoding the domain knowledge that has yielded
these fundamental laws, such as the conservation of momentum or energy,
researchers can ensure the outputs of a machine learning model will have
a physically allowable expression. This encoding can come from
integrating equations, symmetries, or invariances into model design.
These encodings constrain the operation of a machine learning system to
align with the known dynamics of physical systems. The results models
might be expected to produce more accurate results, with smaller
generalisation errors, and with better out-of-distribution
generalisation.

\includegraphics[width=5.52778in,height=5.10021in]{media/image2.png}

\hypertarget{scientific-centaurs}{%
\subsection{Scientific centaurs}\label{scientific-centaurs}}

Complementing modelling strategies to encode scientific knowledge are
deployment strategies to use AI in the lab. The lab has long provided a
physical hub for collaboration and knowledge-generation, its function
and form having remained broadly consistent across centuries of
scientific progress. Today, the digitisation of experimental equipment
and laboratory processes offers opportunities to integrate AI in
experimental design and create new virtual labs.

By combining data from measurement devices, simulations of laboratory
processes, and computational models of research or user objectives,
these virtual labs provide a digital sibling of in-person research
activities that can be used to optimise such activities. In drug
discovery, for example, virtual labs could accelerate the testing and
analysis processes that identify candidate drugs from potential drug
targets. Instead of relying on physical testing of such starting
molecules, multiple rounds of virtual testing can rapidly simulate the
processes of drug design, manufacture, testing, and analysis to assess
which starting molecules are more (or less) likely to be viable
candidate drugs.\footnote{Klami, A., Damoulas, T., Engkvist, O., Rinke,
  P., Kaski, S. (2022): Virtual Laboratories: Transforming research with
  AI. TechRxiv. Preprint.
  \href{https://doi.org/10.36227/techrxiv.20412540.v1}{\uline{https://doi.org/10.36227/techrxiv.20412540.v1}}}
As a result, AI can help accelerate the research process.

Advances in machine learning methods to enable effective simulations,
causal modelling, and encoding pre-existing domain insights -- while
packaging such methods into usable toolkits -- are all necessary
foundations for such digital siblings. Moving from virtual laboratory to
`AI assistants' requires further advances in AI system design to create
AI agents that can elicit guidance or input from their domain experts.
Such agents would not only provide useful intuitions for scientific
modelling, but would serve as `scientific sidekicks', actively helping
researchers to drive their research.

This new type of AI assistant would combine the ability to model the
research problem of interest with the ability to model the goals and
preferences of their expert users, even when the user themselves might
not be able to clearly articulate those goals. As a starting point,
these systems would need to support forms of user interaction that can
extract user knowledge, leveraging this to identify appropriate courses
of action. To operate in contexts where user goals might be uncertain
and user behaviour might change in response to the outputs of the AI
system, these AI sidekicks will need insights from cognitive science,
studies of team decision-making, and new learning strategies based on
limited examples. The sophisticated user modelling so-created would
unlock new forms of human-AI collaboration; scientific centaurs that
combine both human and machine intelligence.\footnote{Celikok, M.M.,
  Oliehoek, F.A., Kaski, S. (2022) Best-Response Bayesian Reinforcement
  Learning with Bayes-Adaptive POMDPs for Centaurs,
  \href{https://arxiv.org/abs/2204.01160}{\uline{arXiv:2204.01160}}\textbf{~}{[}cs.AI{]},
  \href{https://doi.org/10.48550/arXiv.2204.01160}{\uline{https://doi.org/10.48550/arXiv.2204.01160}}}

\hypertarget{enabling-communication-across-domains}{%
\subsection{Enabling communication across
domains}\label{enabling-communication-across-domains}}

Underpinning these efforts to integrate pre-existing knowledge into the
design and deployment of AI systems is a feedback loop between domain
and machine learning research, in which each elicits from and feeds into
the other. This loop requires the ability to exchange knowledge and
insights across disciplines through interdisciplinary collaboration and
communication.

Matching model to user need requires shared understandings of the
research question at hand, the constraints -- whether from data,
compute, funding, or time and energy available -- that affect different
collaborators, and the user needs of the domain environment. While AI
researchers might be tempted to develop complex models, showcasing
assorted theoretical and methodological advances in the field, from a
domain perspective, a relatively `simple' model may seem preferable.
Collaborators need to be able to mutually explore what is possible,
while also considering what is useful.

To complete the loop, outputs from machine learning models need to feed
back into the application domain: insights from AI need to be accessible
in ways that allow the transfer of learning from model to user. This
implies some level of explainability. It is not sufficient for an AI
system to produce highly accurate results; those results must also be
interpretable by a domain researcher. As the complexity of AI systems
increases, however, understanding why these systems have produced a
particular result becomes increasingly challenging. While not an issue
for all machine learning methods, this complexity often results in
difficulties explaining the functioning of AI systems.

In response, AI researchers have developed a variety of different
methods to interrogate how AI systems work, or why a particular output
has been produced. Again, to understand which of these methods is
desirable in the context of a scientific application, researchers must
collaborate closely with domain experts. In the context of
pharmaceutical experiments where the aim is to measure how many target
cells are killed off at different dosages of a drug (or drug
combination), for example, researchers might be seeking to `sense-check'
how different drug dosages affect the model, before investigating
specific drugs more rigorously. In astronomical studies, researchers are
often working with high-dimensional datasets with many confounding
correlations. For example, gravitational waves are ripples in space-time
catalysed by the movement of massive bodies in space, such as planets or
stars.\footnote{NASA, What is a gravitational wave? Available at:
  \href{https://spaceplace.nasa.gov/gravitational-waves/en/}{\uline{https://spaceplace.nasa.gov/gravitational-waves/en/}}}
These invisible phenomena are studied at observatories across the
world,\footnote{See, for example, the LIGO project. Information
  available at:
  \href{https://www.ligo.caltech.edu}{\uline{https://www.ligo.caltech.edu}}}
based on models to describe wave signals and the `noise' generated by
instruments that measure them.\footnote{Dax, M., Green, S.R., Gair, J.,
  Macke, J.H., Buonanno, A. and Schölkopf, B. (2021) Phys. Rev. Lett.
  127, 241103, https://link.aps.org/doi/10.1103/PhysRevLett.127.241103}
Measurements of gravitational waves can be used to infer the properties
of black holes that create them, such as their location, mass, and spin,
using simulation-based inference to characterise the source of a wave,
given the data that detects it. To make such methods more efficient than
existing analytical tools, researchers need to take into account the
structure that sits underneath it: for example, gravitational wave
detectors are located across the globe, and their location affects the
angle at which they detect waves hitting the Earth. This structure can
be exploited through data sampling strategies to help make machine
learning more efficient.\footnote{Ibid.} An alternative, however, is to
use deterministic models that already reflect relevant physical
laws.\footnote{Bodin, E., Dai, Z., Campbell, N.D.F. and Ek, C.H. (2020),
  Black-box density function estimation using recursive partitioning,
  arXiv:2010.13632 {[}stat.ML{]},
  https://doi.org/10.48550/arXiv.2010.13632} Across these approaches,
software packages play an important role in enabling communication and
dissemination of methods for wider use.\footnote{See, for example:
  \href{https://lscsoft.docs.ligo.org/bilby/}{\uline{https://lscsoft.docs.ligo.org/bilby/}}}

\hypertarget{directions-2}{%
\subsection{Directions}\label{directions-2}}

New modelling approaches and mathematical innovations offer exciting
opportunities to integrate domain knowledge, symmetries and invariances
into AI systems.\footnote{Villar, S., Yao, W., Hogg, D.W., Blum-Smith,
  B. and Dumitrascu, B. (2022) Dimensionless machine learning: Imposing
  exact units equivariance, arXiv:2204.00887 {[}stat.ML{]},
  \href{https://doi.org/10.48550/arXiv.2204.00887}{\uline{https://doi.org/10.48550/arXiv.2204.00887}}}
Integration can be achieved in different ways:

\begin{itemize}
\item
  Data augmentation can help exploit invariances and symmetries,
  resulting in improved model performance, by including in the data
  domain knowledge for a model to ingest.
\item
  Symmetries can be embedded in the design of deep learning systems, for
  example by using the same convolutional filters in different locations
  of an image, CNNs can leverage translation and rotation symmetries.
\item
  Latent force models allow representations of known symmetries
  alongside probabilistic factors, enabling integration of mechanistic
  models with unknown forces.\footnote{Ward, W.O.C., Ryder, T., Prangle,
    D. and Álvarez, M.A. (2019) Black-Box Inference for Non-Linear
    Latent Force Models,
    \href{https://arxiv.org/abs/1906.09199}{\uline{arXiv:1906.09199}}
    {[}stat.ML{]},
    \href{https://doi.org/10.48550/arXiv.1906.09199}{\uline{https://doi.org/10.48550/arXiv.1906.09199}}}
\item
  Architectural features can restrict model focus to outputs that
  satisfy symmetries, for example using weight sharing, irreducible
  representations, or invoking symmetries as constraints.\footnote{See,
    for example: Kondor, R. and Trivedi, S. (2018), On the
    Generalization of Equivariance and Convolution in Neural Networks to
    the Action of Compact Groups,
    \href{https://arxiv.org/abs/1802.03690}{\uline{arXiv:1802.03690}}
    {[}stat.ML{]},
    \href{https://doi.org/10.48550/arXiv.1802.03690}{\uline{https://doi.org/10.48550/arXiv.1802.03690}};
    Maron, H., Ben-Hamu, H., and Shamir, N. and Lipman, Y. (2019)
    Invariant and Equivariant Graph Networks,
    \href{https://arxiv.org/abs/1812.09902}{\uline{arXiv:1812.09902}}
    \textbf{{[}cs.LG{]},}
    \href{https://doi.org/10.48550/arXiv.1812.09902}{\uline{https://doi.org/10.48550/arXiv.1812.09902}};
    Dym, N. and Maron, H. (2020) On the Universality of Rotation
    Equivariant Point Cloud Networks,
    \href{https://arxiv.org/abs/2010.02449}{\uline{arXiv:2010.02449}}
    {[}cs.LG{]},
    \href{https://doi.org/10.48550/arXiv.2010.02449}{\uline{https://doi.org/10.48550/arXiv.2010.02449}}}
\item
  Loss functions can be deployed to penalise predictions that fail to
  satisfy physical constraints or symmetries.
\end{itemize}

In the process, emerging mathematical questions include: how can AI
learn invariances from data? And is it possible to quantify the
performance gain achieved through this?

Research to develop AI assistants in the lab raises interesting
questions about learning strategies and human-machine collaboration.
These AI agents would need to be able to learn how to assist another
agent, in a multi-agent decision-making scenario, where goals might be
unclear, uncertain, or changeable. To tackle this challenge:

\begin{itemize}
\item
  Decision-making with delayed reward or zero-shot learning can help
  agents solve tasks when there is little or nothing known about the
  reward function, and no previous behaviour to learn from.
\item
  Interactive knowledge elicitation,\footnote{Sundin, I., Peltola, T.,
    Micallef, L., Afrabandpey, H., Soare, M., Majumder, M.M., Daee, P.,
    He, C., Serim, B., Havulinna, A., Heckman, C., Jacucci, G.,
    Marttinen, P., Kaski, S. (2018) Improving genomics-based predictions
    for precision medicine through active elicitation of expert
    knowledge, \emph{Bioinformatics}, Volume 34, Issue 13, 01 July 2018,
    Pages i395--i403,
    \href{https://doi.org/10.1093/bioinformatics/bty257}{\uline{https://doi.org/10.1093/bioinformatics/bty257}}}
  combining prior knowledge from cognitive science with learning from
  data,\footnote{Kangasrääsiö, A., Jokinen, J.P.P., Oulasvirta, A.,
    Howes, A. and Kaski, S. (2019), Parameter Inference for
    Computational Cognitive Models with Approximate Bayesian
    Computation. Cogn Sci, 43:
    e12738.~\href{https://doi.org/10.1111/cogs.12738}{\uline{https://doi.org/10.1111/cogs.12738}}}
  and generative user models\footnote{De Peuter, S., Oulasvirta, A. and
    Kaski, S. (2021) Toward AI Assistants That Let Designers Design,
    arXiv:2107.13074 {[}cs.HC{]},
    \href{https://doi.org/10.48550/arXiv.2107.13074}{\uline{https://doi.org/10.48550/arXiv.2107.13074}}}
  can support more effective interactions between user and machine.
\end{itemize}

Across these areas, care is needed in the design of the points of
interaction between human and AI system. A core question here is: how
can AI researchers extract domain knowledge from relevant experts and
integrate it into a machine learning model? Insights from human-machine
interaction studies and collaborative decision-making systems are
necessary to create effective interfaces between human and machine,
based on factors such as:

\begin{itemize}
\item
  What forms of visualisation are helpful for human users?
\item
  What types of interpretability or explainability are needed for a user
  to achieve their desired interactions?
\item
  What might be the unintended consequences of human-machine
  interaction, such as over-confidence in results or over-reliance on
  the AI system?
\item
  What `theory of mind' is needed to anticipate how human users might be
  likely to respond to an AI system?
\end{itemize}

A challenge in these interactions is that much of the relevant knowledge
held by the domain expert might be qualitative: an intuition of how a
system works, developed over a long period of study, rather than
quantifiable insights.

\textbf{Box 4: Talks given during this workshop session}

\emph{\textbf{Sami Kaski: Virtual laboratories for science, assisted by
collaborative AI}}

I introduced two ideas: virtual laboratories for science, aiming to
introduce an interface between algorithms and domain science that
enables AI-driven scale advantages, and AI-based `sidekick' assistants,
able to help other agents research their goals, even when they are not
able to yet specify the goal explicitly, or it is evolving. Such
assistants would ultimately be able to help human domain experts run
experiments in the virtual laboratories. I invited researchers to join
the virtual laboratory movement, both domain scientists in hosting a
virtual laboratory in their field and methods researchers in
contributing new methods to virtual laboratories, simply by providing
compatible interfaces in their code. For developing the assistants, I
introduced the basic problem of agents that are able to help other
agents reach their goals, also in zero-short settings, formulated the
problem, and introduced solutions in the simplified setting of prior
knowledge elicitation, and in AI-assistted decision and design tasks.

\emph{\textbf{David Hogg:~Making data analysis more like classical
physics}}

The laws of physics are very structured: They involve coordinate-free
forms, they are equivariant to a panoply of group actions, and they can
be written entirely in terms of dimensionless, invariant quantities. We
find that many existing machine-learning methods can be very
straightforwardly modified to obey the rules that physical law must
obey; physics structure can be implemented without big engineering
efforts. We also find that these modifications often lead to
improvements in generalization, including out-of-sample generalization,
in natural-science contexts. We have some intuitions about why.

The second example is work by Dan Sheldon on analysis of doppler radar
to extract bird biomass and motion. The radar measures the radial
velocity modulo a constant (i.e., the velocity wraps around to zero).
Previous work had attempted to "unwrap" the data using heuristics. Dan
instead incorporated the modulus operation into the likelihood function
and then developing an algorithm for maximizing this somewhat nasty
likelihood. The result has revolutionized radar analysis and has been
deployed in the BirdCast product from the Cornell Lab of Ornithology.

The third example is the species occupancy model introduced by MacKenzie
et al (2002). When human observers conduct wildlife surveys, they may
fail to detect a species even though the species is present. The
occupancy model combines this detection probability with a habitat
model. However, the expressiveness of the two models (detection and
habitat) must be carefully controlled. Rebecca Hutchinson and I learned
this when we tried to replace the linear logistic regression models with
boosted trees.

In all cases, downstream use of the estimates that come from such data
collection models must be aware of the measurement uncertainties. How
can we correctly quantify those uncertainties and incorporate them in
the downstream analysis? Maybe there are lessons ecologists can learn
from physicists?

\emph{\textbf{Mauricio Alvarez: Latent force models}}

A latent force model is a Gaussian process with a covariance function
inspired by a differential operator. Such a covariance function is
obtained by performing convolution integrals between
Green\textquotesingle s functions associated with the differential
operators, and covariance functions associated with latent functions.
Latent force models have been used in several different fields for grey
box modelling and Bayesian inversion. In this talk, I will introduce
latent force models and several recent works in my group where we have
extended this framework to non-linear problems.

\emph{\textbf{Carl Henrik Ek: Translating mechanistic understandings to
stochastic models}}

Statistical learning holds the promise of being the glue that allows us
to improve knowledge parametrised explicitly by a mechanistic model with
implicit knowledge through empirical evidence. Statistical inference
provides a narrative of how to integrate these two sources of
information leading to an explanation of the empirical evidence in
"light" of the explicit knowledge. While the two sources of knowledge
are exchangeable in terms of predictive performance they are not if our
focus is that of statistical learning as a tool for science where we
want to derive new knowledge.

In this talk we will focus on challenges associated with translating our
mechanistic understanding into stochastic models such that they can be
integrated with data. In particular, we will focus on the challenges of
translating composite knowledge. We will show how these structures and
the computational intractabilities they lead to make knowledge discovery
challenging.~

The perceived "success" of machine learning comes from application where
we have large volumes of data such that only simple and generic models
are needed in order to regularise the problem. This means that much of
the progress that have been made with predictive models are challenging
to translate into useful mechanisms for scientific applications. In this
talk we will focus on challenges associated with translating our
mechanistic understanding into stochastic models such that they can be
integrated with data. In specific we will focus on the challenges of
translating composite knowledge. We will show how these structures and
the computational intractabilities they lead to makes knowledge
discovery challenging. We will discuss properties that we desire from
such structures and highlight the large gap that exists with current
inference mechanisms.

\hypertarget{a-research-agenda-in-ai-for-science}{%
\section{A research agenda in AI for
science}\label{a-research-agenda-in-ai-for-science}}

`AI for science' sits at a nexus of disciplines, methods, and
communities. Both AI and `science' (broadly defined) share a core
interest in learning from data. From this interest emerge different
research directions: for AI, questions about the nature of intelligence
and how to understand the learning process in humans and machines; for
science, the outputs of this learning process are the focus, with the
aim of adding new knowledge about natural, physical, and social systems.
A distinctive feature of the emerging `AI for science' agenda is the
ability to move between these worlds, using AI to drive progress in
science and taking inspiration from science to inspire progress in AI.
The result is a continuum of modelling approaches along a spectrum from
strongly mechanistic to statistical models, which allow researchers to
introduce or operate at different levels of abstraction.

The AI for science community therefore combines the ambitions of AI
research with domain-specific goals to advance the frontiers of research
and innovation in their discipline, with an engineering focus on
designing systems that work in deployment, while operating across scales
from the nano- to the interstellar. From these interfaces emerges a
research agenda that -- if successful -- promises to accelerate progress
across disciplines. Inspired by discussions at the Dagstuhl workshop, a
list of research questions arising from this agenda is given in Annex 2.
These span three themes:

\emph{\textbf{Building AI systems for science:}} Attempts to deploy AI
in the context of scientific discovery have exposed a collection of gaps
in current machine learning and AI capabilities. Further work is needed
to develop the technical capabilities that will allow AI to be used more
effectively in research and innovation; developing those capabilities
also offers opportunities to contribute to wider attempts to deliver
sophisticated AI systems. Areas for progress include:

\begin{itemize}
\item
  Advancing methods, software and toolkits for high-quality simulation
  and emulation, which integrate effective uncertainty quantification
  and leverage advances in machine learning robustness to ensure the
  operate safely and effectively.
\item
  Detecting scientifically meaningful structure in data, through
  advances in causal machine learning.
\item
  Encoding domain knowledge in AI systems through integration of
  scientific laws, principles, symmetries, or invariances in machine
  learning models, and through virtual, autonomous systems to make
  research more effective.
\end{itemize}

\emph{\textbf{Combining human and machine intelligence:}} Effective
deployment of AI in science requires effective interactions between
human, domain and machine intelligence across all stages of the
deployment pathway. AI systems can be made more effective by integrating
pre-existing knowledge about the system of study, but mechanisms are
needed to extract and encode that knowledge. Effective interfaces are
also required in the reverse direction. Translating the outputs of AI
analysis to increased human capability requires an understanding of what
insights are relevant, how they are best communicated, and the cultural
environment that shapes the conduct of science. Areas for progress
include:

\begin{itemize}
\item
  Designing interfaces between humans and machines or AI agents that can
  extract, formalise, and assimilate knowledge that domain researchers
  have acquired, including tacit knowledge, and that communicate new
  knowledge back to the user as actionable insights.
\item
  Building mechanisms for explainability that allow researchers to
  interrogate why and how an AI system delivered a particular result,
  with the explanations provided being tailored to user need.
\item
  Accelerating the pace of knowledge creation and use, through systems
  that mine the existing research knowledge base or that automate
  repetitive or time-consuming elements of the research process.
\end{itemize}

\emph{\textbf{Influencing practice and adoption:}} By learning from
recent experiences of deploying AI for science, the field has an
opportunity to promote wider uptake and progress in both scientific
domains and in AI research. This requires capturing both the knowledge
that the community has already generated, about how to design AI
systems, and the know-how about how to overcome practical challenges
that accompanies it, while taking action to grow the community of
researchers excited about the potential of AI in science. Areas for
progress include:

\begin{itemize}
\item
  Supporting new applications, through challenge-led research programmes
  that promote interdisciplinary collaborations and support co-design of
  AI systems to help tackle scientific challenges.
\item
  Developing toolkits and user guides that allow researchers to
  understand which AI tools are suitable for which purposes, and how to
  deploy those tools in practice.
\item
  Sharing skills and know-how, through community outreach that
  disseminates knowledge and know-how in how to use AI.
\end{itemize}

Together, these areas for action highlight the importance of interfaces
-- between researchers and between modelling approaches -- in shaping
the development of AI for science (Figure 3).

\includegraphics[width=5.98951in,height=5.6132in]{media/image3.png}

\hypertarget{accelerating-progress-in-ai-for-science}{%
\section{\texorpdfstring{Accelerating progress in AI for science
}{Accelerating progress in AI for science }}\label{accelerating-progress-in-ai-for-science}}

Building on the impressive advances that machine learning has already
supported in many domains, widespread adoption of AI for research has
the potential to catalyse a new wave of innovations that in turn could
drive greater health, wealth, and wellbeing. The question facing
researchers, funders, and policymakers today is how to harness that
potential. The challenge is to build capability across the research
landscape, connect areas of expertise to areas of need, and to
accelerate the transfer of successful ideas between domains.

The experiences of deploying AI for science described in this document,
and the research agenda that results from these experiences, suggest a
roadmap for action. That roadmap charts a pathway to create an enabling
environment for AI in science, by advancing research that delivers AI
methods to support scientific discovery, building tools and resources to
make AI accessible, championing interdisciplinary research and the
people pursuing it, and nurturing a community at the interface of these
different domains. Progress across these areas can unlock scientific and
methodological advances in AI for science, while also helping answer an
emerging question about whether there exists a core discipline of `AI
for science'. The shared themes and interests that emerge from research
projects at the interface of AI and scientific domains suggest that
there is potential for `AI for science' to surface as a distinct
speciality in computer science. In parallel, domain-specific efforts to
drive the adoption of AI as an enabler of innovation are also needed to
deliver the benefits of AI for scientific discovery.

\hypertarget{advance-new-methods-and-applications}{%
\subsection{Advance new methods and
applications}\label{advance-new-methods-and-applications}}

Efforts to deploy AI in the context of research have highlighted
cross-cutting challenges where further progress in AI methods and theory
is needed to create tools that can be used more reliably and effectively
in the scientific context. Effective simulations are needed to study the
dynamics of complex systems; causal methods to understand why those
dynamics emerge; and integration of domain knowledge to relate those
understandings to the wider world. While elements of these research
challenges are shared with other fields -- topics such as robustness,
explainability, and human-machine interaction also come to the fore in
fields such as AI ethics, for example -- they share an intersection in
the use of AI for science, in the context of efforts to bridge
mechanistic and data-driven modelling.

Alongside these `AI' challenges are a collection of `science'
challenges, where researchers, policymakers and publics have aspirations
for AI to deliver real-world benefits.\footnote{See, for example: the
  EU's Innovation Missions
  \href{https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-europe/eu-missions-horizon-europe_en}{\uline{https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-europe/eu-missions-horizon-europe\_en}}
  and UN SDG's
  \href{https://sdgs.un.org/goals}{\uline{https://sdgs.un.org/goals}}}
Such challenges offer the opportunity to accelerate progress in AI,
while facilitating interdisciplinary exchanges, and opening the field to
input from citizen science or other public engagement initiatives. In
developing these research missions, care is needed to define
cross-cutting questions or challenges that broaden scientific
imaginations, rather than restricting them. The process of converting a
complicated scientific problem into something tractable with AI
necessarily involves some narrowing of focus; to be successful,
mission-led innovation efforts must achieve this focus without losing
meaning, or creating benchmarks that misrepresent the complexity of the
real-world challenge.

Defining shared challenges could help rally the AI for science community
and drive progress in both methods and applications of AI in science.
There already exists examples of how such challenges can build
coalitions of researchers across domains from which the field can draw
inspiration. These include the GREAT08 project, which developed image
analysis techniques to study gravitational lensing;\footnote{Bridle, S.,
  Balan, S.T., Bethge, M., Gentile, M., Harmeling, S., Heymans, C.,
  Hirsch, M., Hosseini, R., Jarvis, M., Kirk, D., Kitching, T., Kuijken,
  K., Lewis, A., Paulin-Henriksson, S., Schölkopf, B., Velander, M.,
  Voigt, L., Witherick, D., Amara, A., Bernstein, G., Courbin, F., Gill,
  M., Heavens, A., Mandelbaum, R., Massey, R., Moghaddam, B., Rassat,
  A., Réfrégier, A., Rhodes, J., Schrabback, T., Shawe-Taylor, J.,
  Shmakova, M., van Waerbeke, L., Wittman, D. (2010) Results of the
  GREAT08 Challenge: an image analysis competition for cosmological
  lensing, \emph{Monthly Notices of the Royal Astronomical Society},
  Volume 405, Issue 3, July 2010, Pages 2044--2061,
  \href{https://doi.org/10.1111/j.1365-2966.2010.16598.x}{\uline{https://doi.org/10.1111/j.1365-2966.2010.16598.x}}}
the Open Problems in Single Cell Biology challenge, which convened the
machine learning community to make progress in Multimodal Single-Cell
Data Integration;\footnote{For further information, see:
  https://openproblems.bio/neurips\_2021/} and the SENSORIUM challenge,
focused on advancing understandings of how the brain processes visual
inputs.\footnote{For further information, see:
  https://sensorium2022.net/home} In pursuing this agenda, researchers
can leverage well-established protocols in open-sourcing materials and
sharing documentation to help ensure research advances are rapidly and
effectively disseminated across disciplines. The result should be more
effective methods, and an agile research environment where researchers
can flex methods across disciplines.

\hypertarget{invest-in-tools-and-toolkits}{%
\subsection{Invest in tools and
toolkits}\label{invest-in-tools-and-toolkits}}

Complementing these efforts to build and share knowledge, well-designed
software tools can help make accessible the craft skills (or know-how)
that make AI for science projects successful. Modelling is a core
component of all AI for science projects. In some aspects, the task for
the field can be thought of as charting a path between the statistician,
whose effectiveness comes from proximity to the domain but whose methods
struggle to scale, and the mathematician, whose tools are adopted across
domains but with some loss of meaning as the distance between
method-generator and adopter increases.

The energy already invested in building effective machine learning
models can be leveraged for wider progress across domains through
investment in toolkits that support the generalisation of effective
approaches. Wide-spectrum modelling tools could offer `off the shelf'
solutions to common AI for science research questions. The challenge for
such toolkits is to create an effective interface between tool and user.
Connecting with the field of human-computer interaction could generate
design insights or protocols to help create more effective human-AI
interfaces.

Best practices in software engineering can help, through documentation
that supports users to successfully deploy modelling tools. User guides
-- or taxonomies of which models are best suited for which purposes and
under what circumstances -- can also help make accessible to non-expect
users the accumulated know-how that machine learning researchers have
gained through years of model development and deployment.

A related engineering challenge is that of data management and
pipeline-building. To interrogate how a model works, why a result was
achieved, or whether an AI system is working effectively, researchers
often benefit from being able to track which data contributed to which
output. The data management best practices that allow such tracking need
to be embedded across AI for science projects. Data management
frameworks -- such as the FAIR data principles -- have already been
developed with the intention of making data more available, and useful,
for research. Further investment is now needed in efforts to implement
those principles in practice.

Investment in these foundational tools and resources can help build
understanding of which AI methods can be used and for what purposes,
lowering the barriers to adopting AI methods across disciplines.

\hypertarget{build-capability-across-disciplines}{%
\subsection{Build capability across
disciplines}\label{build-capability-across-disciplines}}

Central to progress in both research and toolkit engineering is the
availability of talented researchers with a passion for advancing
science through AI. People matter at all stages of the AI development
and deployment pipeline. Successful projects rely on researchers who are
motivated to work at the interface of different domains; collaborators
who can explain and communicate core concepts in their work across
disciplinary boundaries; engineers who can translate the needs of
different users into AI toolkits; and convenors that can inspire wider
engagement with the AI for science agenda.

Building these capabilities requires multiple points of engagement.
Domain researchers need access to learning and development activities
that allow them to understand and use foundational methods in machine
learning, whether as formal training or through the availability of
tutorials or user guides. AI researchers need access to the scientific
knowledge that should shape the methods they develop, the skills to
translate their advanced knowledge to materials that can be shared for
wider use, and the capacity to dedicate time and resource to learning
about domain needs.\footnote{A comparison here can be drawn with the
  development of statistics as an enabling discipline for many domains:
  statisticians have devoted time to understanding domain practices and
  integrating their work within those practices, often dedicating
  significant resource to understand the nature of the datasets with
  which they are working, before introducing modelling ideas.} Both need
skills in communication, organisation, and convening to operate across
disciplines. Without such capability-building, disciplines risk
remaining siloed; domains developing unrealistic expectations about what
AI can deliver in practice, and AI losing touch with the scientific
questions that are most meaningful to domains.

Institutional incentives shape how individuals engage (or not) with such
interdisciplinary exchanges. Interdisciplinary research often takes
longer and lacks the outlets for recognition available to those working
in single domains, affecting both the motivation of and opportunities
for career progression that are open to those working at the interface
of different disciplines. Much of the engineering work required to make
data and AI accessible beyond a specific project and useful to a wider
community is also traditionally unrecognised by academic incentive
structures. Aligning individual and institutional incentives in support
of interdisciplinarity is a long-standing challenge in research, and one
that becomes more critical to address in the context of developments in
AI. In this context, there may be new opportunities to recognise and
reward successes in AI for science, whether through new fellowships,
prizes, or ways of promoting the work done by those at this interface.

\hypertarget{grow-communities-of-research-and-practice}{%
\subsection{Grow communities of research and
practice}\label{grow-communities-of-research-and-practice}}

The areas for action described above feed into and from each other.
Progress in research and application can be leveraged to inspire a
generation of researchers to pursue interdisciplinary projects;
effective toolkits can make such progress more likely; skills-building
initiatives can prime researchers to be able to use these toolkits; and
so on, to create an environment where researchers and research advances
transition smoothly across disciplines, leading to a rising AI tide that
lifts all disciplines. Communities of research and practice are the
backdrop for creating such positive feedback loops.

A collection of AI for science initiatives are already building links
across the research landscape. The Machine Learning for Science Cluster
of Excellence at the University of Tübingen is leveraging the strength
of its local ecosystem in AI to drive wider progress in research and
innovation;\footnote{Programme website available at:
  \href{https://uni-tuebingen.de/en/research/core-research/cluster-of-excellence-machine-learning/home/}{\uline{https://uni-tuebingen.de/en/research/core-research/cluster-of-excellence-machine-learning/home/}}}
the Accelerate Programme for Scientific Discovery at the University of
Cambridge is building bridges across disciplines, building a community
passionate about opportunities in AI for science;\footnote{Programme
  website available at:
  \href{https://acceleratescience.github.io}{\uline{https://acceleratescience.github.io}}}
the University of Copenhagen's SCIENCE AI Centre provides a focal point
for AI research and education in its Faculty for Science;\footnote{Programme
  website available at:
  \href{https://ai.ku.dk}{\uline{https://ai.ku.dk}}} New York
University's Center for Data Science hosts interdisciplinary faculty
pursuing innovative research and education;\footnote{Programme website
  available at: \href{https://cds.nyu.edu}{\uline{https://cds.nyu.edu}}}
the University of Wisconsin-Madison's American Family Insurance Data
Science Institute is developing strategic partnerships to accelerate the
use of data science in research;\footnote{Programme website available
  at:
  \href{https://datascience.wisc.edu/institute/}{\uline{https://datascience.wisc.edu/institute/}}}
new investments by Schmidt Futures across a network of research
institutions are supporting new postdoctoral fellowships at the
interface of AI and sciences.\footnote{Schmidt Futures (2022) Schmidt
  Futures Launches \$148M Global Initiative to Accelerate AI Use in
  Postdoctoral Research, available at:
  \href{https://www.schmidtfutures.com/schmidt-futures-launches-148m-global-initiative-to-accelerate-ai-use-in-postdoctoral-research/}{\uline{https://www.schmidtfutures.com/schmidt-futures-launches-148m-global-initiative-to-accelerate-ai-use-in-postdoctoral-research/}}}
Together, these initiatives demonstrate the appetite for progress in AI
for science.

There is an opportunity today to leverage these emerging interests into
a wider movement. Existing initiatives can drive capability-building, by
making training and user guides open, reaching out to engage domain
researchers in skills-building activities, and fostering best practice
in software and data engineering across disciplines. The links they
establish across research domains can form the basis of new
communication channels, whether through discussion forums, research
symposia, or newsletters to share developments at the interface of AI
and science. These communications can be deployed to raise the profile
of people and projects at this interface, celebrating successes, sharing
lessons, and demonstrating the value of interdisciplinary work.
Together, they can help develop an infrastructure for AI in science.

That infrastructure may also benefit from new institutional
interventions to address long-standing challenges in interdisciplinary
AI. New journals could provide an outlet to publish and recognise
high-quality AI for science research, bringing in contributions from
multiple disciplines and helping translate lessons across areas of work.
Membership organisations could help foster a sense of belonging and
community for researchers working at the interface of AI, science, and
engineering, developing career pathways and incentives. Efforts to
convene across disciplines can also catalyse new connections and
collaborations.

Emerging from these efforts is a paradigm shift in how to drive progress
in science. Historically, a small number of foundational texts have been
the catalyst that changed how researchers studied the world; Newton's
Principia; Darwin's Origin of Species; and so on. For much of its modern
history, scientific knowledge has been transmitted through textbooks;
canonical descriptions of the current state of knowledge. Today, the
transformative potential of AI is driven by its pervasiveness; its
impact in science will be achieved through integration across
disciplines. This integration requires widespread mobilisation,
convening machine learning researchers, domain experts, citizen
scientists, and affected communities to shape how AI technologies are
developed and create an amenable environment for their deployment. It
takes a community.

\hypertarget{ai-and-science-building-the-interface}{%
\subsection{AI and science: building the
interface}\label{ai-and-science-building-the-interface}}

Advances in AI have disrupted traditional ways of thinking about
modelling in science. Where researchers might previously have
conceptualised models as mechanistic -- reflecting known forces in the
world -- or data-driven, the `AI for science' methods that are emerging
today reject this separation. They are both, combining insights from
mechanistic and data-driven methods, integrating methods to create
something new. What follows from these developments is a spectrum of
modelling approaches, which researchers can deploy flexibly in response
to the research question of interest.

Today, the field of AI for science is characterised by intersections.
Between AI and scientific domains; between science and engineering;
between knowledge and know-how; between human and machine. It operates
across disciplinary boundaries, across scales from the atomic to the
universal, and across both the mission to understand intelligence and
the quest to deploy human intelligence to understand the world. Emerging
from these missions is a continuum of models and methods that allow
researchers to work across domains, extracting the knowledge that humans
have acquired, and levels of inquiry, enhancing that knowledge and
returning it in actional form.

As both a domain itself and an enabler of other disciplines, the power
of AI in science lies in its ability to convene diverse perspectives in
ways that accelerates progress across research areas. AI for science is
a rendezvous point. Its next wave of development will come from taking
strength from its diversity, and bringing more people into its
community.

\hypertarget{annex-1-participants-at-machine-learning-for-science-bridging-data-driven-and-mechanistic-modelling-dagstuhl-seminar-22382-18-23-september-2022}{%
\section{Annex 1: Participants at Machine Learning for Science: Bridging
Data-driven and Mechanistic Modelling (Dagstuhl Seminar 22382, 18-23
September
2022)}\label{annex-1-participants-at-machine-learning-for-science-bridging-data-driven-and-mechanistic-modelling-dagstuhl-seminar-22382-18-23-september-2022}}

\emph{Organisers: Philipp Berens, Kyle Cranmer, Neil Lawrence, Jessica
Montgomery, Ulrike von Luxburg.}

Thank you to all those who contributed to Dagstuhl Seminar 22382,
discussions at which are the foundation for this paper:

Mauricio A Álvarez (University of Manchester, GB)

Bubacarr Bah (AIMS South Africa -- Cape Town, ZA)

Jessica Beasley (Collective Next -- Boston, US)

Philipp Berens (Universität Tübingen, DE)

Maren Büttner (Helmholtz Zentrum München \& Universität Bonn)

Kyle Cranmer (University of Wisconsin -- Madison, US)

Thomas G. Dietterich (Oregon State University -- Corvallis, US)

Carl Henrik Ek (University of Cambridge, GB)

Stuart Feldman (Schmidt Futures -- New York, US)

Asja Fischer (Ruhr-Universität Bochum, DE)

Philipp Hennig (Universität Tübingen, DE)

David W. Hogg (New York University, US)

Christian Igel (University of Copenhagen, DK)

Samuel Kaski (Aalto University, FI)

Ieva Kazlauskaite (University of Cambridge, GB)

Hans Kersting (INRIA -- Paris, FR)

Niki Kilbertus (TU München, DE \& Helmholtz AI München, DE)

Neil D. Lawrence (University of Cambridge, GB)

Gilles Louppe (University of Liège, BE)

Jakob Macke (Universität Tübingen, DE)

Dina Machuve (DevData Analytics -- A, TZ)

Eric Meissner (University of Cambridge, GB)

Siddharth Mishra-Sharma (MIT -- Cambridge, US)

Jessica Montgomery (University of Cambridge, GB)

Jonas Peters (University of Copenhagen, DK)

Aditya Ravuri (University of Cambridge, GB)

Markus Reichstein (MPI für Biogeochemistry -- Jena, DE)

Bernhard Schölkopf (MPI für Intelligente Systeme -- Tübingen, DE)

Francisco Vargas (University of Cambridge, GB)

Soledad Villar (Johns Hopkins University -- Baltimore, US)

Ulrike von Luxburg (Universität Tübingen, DE)

Verena Wolf (Universität des Saarlandes -- Saarbrücken, DE)

The Accelerate Programme for Scientific Discovery would like to thank
Schmidt Futures for its continuing support, and the donation that
enables its work.

\protect\hypertarget{_Toc121762051}{}{}\textbf{Annex 2: Research
questions arising from the `AI for science research agenda' discussion
during the Dagstuhl workshop}

Building AI systems for science

\begin{itemize}
\item
  How can AI systems accurately generalise from finite observations? How
  can they detect causality or structure from finite observations?
\item
  What is the computational cost of complexity, and what methods can
  help manage this?
\item
  What forms of system calibration and uncertainty quantification are
  useful in the context of scientific discovery? Are theoretical
  guarantees necessary?
\item
  What new forms of explainability or interpretability could facilitate
  the deployment of AI in science?
\item
  How could AI support generalisation from a small number of
  observations? What methods could enable few- or one-shot learning?
\item
  How can AI researchers build meaningful models from data to accurately
  represent causal mechanisms in the system of study? How can
  researchers identify the most effective model for their system of
  study?
\item
  What does it mean to understand a model? How can researchers combine
  explainability with complexity?
\item
  How can AI methods be made robust and easy to use in deployment by
  domain scientists?
\item
  How can advances in simulation methods be applied in domains where the
  system at hand is less easily described by equations?
\item
  What advances are needed to expand the use of simulations in science?
  How can AI help simulate laboratory experiments or environments,
  helping make more efficient different elements of the scientific
  process? How might this be expanding in the long-term, for example to
  planning experimental design or helping identify where data is
  missing?
\item
  How can `digital siblings' be used to explore the impact of different
  interventions on complex systems?
\end{itemize}

Combining human and machine intelligence

\begin{itemize}
\item
  How can AI researchers best extract, formalise and assimilate the
  knowledge that domain researchers have acquired? What forms of
  knowledge representation can formalise scientific understandings of
  the world, translating these to objective functions for AI systems?
  What forms of human-AI engagement can make use of the `qualitative'
  knowledge -- or intuitions about a system -- that domain researchers
  have accumulated?
\item
  How can AI capture the qualitative understanding that researchers have
  of their domain to more accurately or effectively characterise a
  system?
\item
  How can AI be effectively deployed to mine the existing research
  knowledge base -- for example, papers, databased, and so on -- to
  extract new insights?
\item
  Where can automation support research progress? Which elements of the
  scientific process could be automated, and where is human input vital?
\item
  What forms of collaboration are needed to effectively specify helpful
  outputs from an AI system?
\item
  How can insights from AI analysis be returned to researchers in an
  actionable way? What mix of AI design, engineering, social
  interaction, and education can make effective interfaces between
  domain researchers and AI systems?
\item
  How can the outputs of AI systems be made interpretable for scientific
  users?
\item
  How can AI researchers better understand and design for the forms of
  interpretability that resonate with domain researchers?
\item
  What processes of collaboration or co-design can help describe what
  scientists `need to know from an AI system?
\item
  What best practices or methods can be deployed to effectively
  communicate uncertainty from AI systems to human users?
\end{itemize}

Influencing practice and adoption

\begin{itemize}
\item
  What are the craft skills in AI for science? What `know how' is
  necessary to make AI work effectively in practice?
\item
  What skills-building or forms of outreach can help take AI tools out
  of the AI community and into `the lab'?
\item
  How has machine learning been used most effectively for research and
  innovation? What best practices, or lessons, do existing efforts in AI
  for science offer?
\item
  Which AI tools are suitable for which purposes, disciplines, or
  experimental designs? Is it possible to create a taxonomy for science?
\item
  Are there generalisable methods or conclusions that can be taken from
  domain-specific efforts to deploy AI for science?
\end{itemize}
